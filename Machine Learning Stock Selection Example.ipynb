{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4779c39c",
   "metadata": {},
   "source": [
    "# 量化投资第一次作业\n",
    "**作者：** 黄倪远\n",
    "\n",
    "**学号：** 3200101028\n",
    "\n",
    "**班级：** 金融学2003\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "1.\t原始代码为静态训练，可否改成滚动训练，如每次使用最近N个月数据训练，每半年更新一次模型？（提示：增加循环即可）\n",
    "    * 在Para类中增加`scroll_interval`和`scroll_times`表示滚动的间隔和滚动的次数在之后的训练中增加大循环以实现更新模型的目的。\n",
    "\n",
    "\n",
    "2.\t原始代码未涉及交叉验证调参，验证集未实际发挥作用，可否添加调参模块？（提示：sklearn有现成工具）\n",
    "    * 改用k-Folder交叉检验方法；\n",
    "    * 在Para类中定义`evaluation_metrics`来表示检验的方法，`cv`来表示K值；\n",
    "        * 具体代码见第五个代码块的第109-122行\n",
    "    * 利用网络搜索进行调参\n",
    "        * 具体代码见第五个代码块的第124-203行（由于运行时间太长，把这部分代码注释掉了）\n",
    "\n",
    "\n",
    "3.\t原始代码仅使用单一模型训练，其他机器学习方法表现如何，这些方法内部相关性如何？可否对低相关模型进行集成，进一步提升表现？（提示：绝大部分方法sklearn有现成工具；神经网络可使用tf、pytorch等编写，有一定难度）\n",
    "    * 采用Mean Squared Error、Median Absolute Error、R方作为衡量内部相关性的指标；\n",
    "        * 具体代码见第五个代码块的第205-218行；\n",
    "    * 在第四个代码块中定义了别的模型：KNN、SVM、Logistic Regression(LR)、Random Forest；\n",
    "    * 在第六个代码块之后，尝试搭建了一下模型融合的方法，由于运行耗时实在太大，没有能将结果输出，可能中间的代码细节有诸多纰漏，大致思路如下：\n",
    "        * 借鉴k-FOLDER的思路，来决定不同模型之间的决策边界；\n",
    "        * 再利用多个模型的结果进行投票产生结果。\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "4.\t原始策略在2017年后经历了较长的超额收益回撤（相对沪深300），可能是什么原因？有什么方法可以改进？  \n",
    "\n",
    "    该模型本质上是基于历史经验信息，对未来市场的预测本身就存在一定的风险，而在2017年之后的较大回撤可能有以下的原因：\n",
    "    * 训练集的各个样本在时间维度上是等权的，这显然不符合股市中时间越近影响越大的实际场景。可以在模型中加入时间尺度，给时间设置权重参数或者使用时间序列学习模型；\n",
    "    * 模型的准确率仍然较低。模型存在很大改进空间，提升Xgboost损失函数精度，准确的将错分类的惩罚力度区分开来。\n",
    "    * 对模型中使用的因子不够全面。给模型中加入更多稳健的因子，以解决模型在部分年份失效的问题。\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58ea4088",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 第一题：滚动训练：如每次使用最近N个月数据训练，每半年更新一次模型\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5b3957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 定义参数类\n",
    "#-- define a class including all parameters\n",
    "class Para():\n",
    "    \n",
    "    method = 'XGBOOST-C' # 'LOGI' 'XGBOOST-C' 'LR'\n",
    "    month_in_sample = range(82,144+1) #-- return 82~144 63 months（样本内）\n",
    "    month_test = range(145,269+1) #-- return 145~269 125 months（样本外）\n",
    "    \n",
    "    scroll_interval = 6 #每隔6个月更新一次\n",
    "    scroll_times = range(0,4) #总共更新了4次\n",
    "    \n",
    "    evaluation_metrics = 'cross_val_score' #'cross_val_score' 'cross_validate'\n",
    "    cv = 5 # k value in k-folder\n",
    "    \n",
    "    percent_select = [0.3,0.3] #-- 30% positive samples, 30% negative samples\n",
    "    percent_cv = 0.1 #-- percentage of cross validation samples （交叉验证样本的百分比）\n",
    "    \n",
    "    path_data = './data/csv_demo/'\n",
    "    path_results = './results/'\n",
    "    \n",
    "    seed = 42 #-- random seed\n",
    "    \n",
    "    logi_c = 0.0006 #-- logistic regression parameter\n",
    "    \n",
    "    xgbc_n_estimators = 100 #-- xgboost classifier parameter\n",
    "    xgbc_learning_rate = 0.1 #-- xgboost classifier parameter\n",
    "    xgbc_subsample_C = 0.95 #-- xgboost classifier parameter\n",
    "    xgbc_max_depth = 3 #-- xgboost classifier parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cba9e8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 生成二分类标签函数\n",
    "#-- function, label data \n",
    "def label_data(data):\n",
    "    #-- label data\n",
    "    data['return_bin'] = np.nan #nan就是NAN，缺失值\n",
    "    \n",
    "    #-- sort by return\n",
    "    data = data.sort_values(by='return',ascending=False) #按照return字符对数据进行降序排序\n",
    "    \n",
    "    #-- decide the amount of stocks selected\n",
    "    n_stock_select = np.multiply(para.percent_select,data.shape[0]) #求内积：para.percent_select=[0.3,0.3]是选择的百分比 shape[0]应该是样本总量\n",
    "    n_stock_select = np.around(n_stock_select).astype(int) #取整\n",
    "    \n",
    "    #-- assign 1 or 0 在最后加一列，前百分之30赋1 后百分之30赋0\n",
    "    data.iloc[0:n_stock_select[0],-1] = 1\n",
    "    data.iloc[-n_stock_select[1]:,-1] = 0\n",
    "    \n",
    "    #-- remove other stocks\n",
    "    data = data.dropna(axis=0)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcb1e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_module(x,y,indices, c_param, bdry=None):\n",
    "    knn=KNeighborsClassifier(n_neighbors=c_param)\n",
    "    knn.fit(x.iloc[indices[0],:], y.iloc[indices[0],:].values.ravel())\n",
    "    y_pred_undersample = knn.predict(x.iloc[indices[1],:].values)\n",
    "    \n",
    "    return y_pred_undersample\n",
    "    \n",
    "def svm_rbf_module(x, y, indices, c_param, bdry= 0.5):\n",
    "    svm_rbf = SVC(C=c_param, probability=True)\n",
    "    svm_rbf.fit(x.iloc[indices[0],:], y.iloc[indices[0],:].values.ravel())\n",
    "    y_pred_undersample = svm_rbf.predict_proba(x.iloc[indices[1],:].values)[:,1] >= bdry\n",
    "    return y_pred_undersample\n",
    "\n",
    "def svm_poly_module(x,y, indices, c_param, bdry=0.5):\n",
    "    svm_poly=SVC(C=c_param[0], kernel='poly', degree= c_param[1], probability=True)\n",
    "    svm_poly.fit(x.iloc[indices[0],:], y.iloc[indices[0],:].values.ravel())\n",
    "    y_pred_undersample = svm_poly.predict_proba(x.iloc[indices[1],:].values)[:,1] >= bdry\n",
    "    return y_pred_undersample\n",
    "\n",
    "def lr_module(x,y, indices, c_param, bdry=0.5):\n",
    "    lr = LogisticRegression(C=c_param,penalty='11')\n",
    "    lr.fit(X.iloc[indices[0],:], y.iloc[indices[0],:].values.ravel())\n",
    "    y_pred_undersample= lr.predict_proba(X.iloc[indices[1],:].values)[:,1]>=bdry\n",
    "    return y_pred_undersample\n",
    "    \n",
    "def rf_module(x,y, indices, c_param, bdry=0.5):\n",
    "    rf= RandomForestClassifier(n_jobs=-1,n_estimators=100, criterion='entropy', max_features= 'auto',\n",
    "                               max_depth=None,min_samples_split= c_param, random_state=0)\n",
    "    rf.fit(X.iloc[indices[0],:], y.iloc[indices[0],:].values.ravel())\n",
    "    y_pred_undersample = rf.predict_proba(X.iloc[indices[1],:].values)[:,1]>=bdry\n",
    "    return y_pred_undersample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "947b9a3f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASIC INFORMATION:\n",
      "method = XGBOOST-C\n",
      "evaluation_metrics = cross_val_score\n",
      "scroll times = 4\n",
      "------------------------------------\n",
      "\n",
      "training times = 1\n",
      "##----------------------------------\n",
      "1.CROSS-VALIDATION:\n",
      "[0.58573148 0.58901412 0.58802932 0.59787723 0.58065222]\n",
      "##----------------------------------\n",
      "2.INTERNAL DEPENDENCE:\n",
      "Mean Squared Error(MSE) = 0.240088\n",
      "Median Absolute Error(MAE) = 0.478789\n",
      "R^2 score = 0.039487\n",
      "##----------------------------------\n",
      "3.MODOLE EVALUATION:\n",
      "training set, accuracy = 0.62\n",
      "training set, AUC = 0.66\n",
      "cv set, accuracy = 0.58\n",
      "cv set, AUC = 0.61\n",
      "testing set, month 145, accuracy = 0.48\n",
      "testing set, month 145, AUC = 0.48\n",
      "testing set, month 146, accuracy = 0.56\n",
      "testing set, month 146, AUC = 0.58\n",
      "testing set, month 147, accuracy = 0.64\n",
      "testing set, month 147, AUC = 0.69\n",
      "testing set, month 148, accuracy = 0.51\n",
      "testing set, month 148, AUC = 0.54\n",
      "testing set, month 149, accuracy = 0.55\n",
      "testing set, month 149, AUC = 0.58\n",
      "testing set, month 150, accuracy = 0.51\n",
      "testing set, month 150, AUC = 0.51\n",
      "testing set, month 151, accuracy = 0.58\n",
      "testing set, month 151, AUC = 0.63\n",
      "testing set, month 152, accuracy = 0.53\n",
      "testing set, month 152, AUC = 0.55\n",
      "testing set, month 153, accuracy = 0.65\n",
      "testing set, month 153, AUC = 0.70\n",
      "testing set, month 154, accuracy = 0.51\n",
      "testing set, month 154, AUC = 0.52\n",
      "testing set, month 155, accuracy = 0.60\n",
      "testing set, month 155, AUC = 0.65\n",
      "testing set, month 156, accuracy = 0.52\n",
      "testing set, month 156, AUC = 0.54\n",
      "testing set, month 157, accuracy = 0.54\n",
      "testing set, month 157, AUC = 0.58\n",
      "testing set, month 158, accuracy = 0.55\n",
      "testing set, month 158, AUC = 0.57\n",
      "testing set, month 159, accuracy = 0.54\n",
      "testing set, month 159, AUC = 0.57\n",
      "testing set, month 160, accuracy = 0.52\n",
      "testing set, month 160, AUC = 0.55\n",
      "testing set, month 161, accuracy = 0.59\n",
      "testing set, month 161, AUC = 0.64\n",
      "testing set, month 162, accuracy = 0.55\n",
      "testing set, month 162, AUC = 0.58\n",
      "testing set, month 163, accuracy = 0.57\n",
      "testing set, month 163, AUC = 0.60\n",
      "testing set, month 164, accuracy = 0.50\n",
      "testing set, month 164, AUC = 0.52\n",
      "testing set, month 165, accuracy = 0.60\n",
      "testing set, month 165, AUC = 0.63\n",
      "testing set, month 166, accuracy = 0.56\n",
      "testing set, month 166, AUC = 0.58\n",
      "testing set, month 167, accuracy = 0.61\n",
      "testing set, month 167, AUC = 0.64\n",
      "testing set, month 168, accuracy = 0.56\n",
      "testing set, month 168, AUC = 0.58\n",
      "testing set, month 169, accuracy = 0.57\n",
      "testing set, month 169, AUC = 0.60\n",
      "testing set, month 170, accuracy = 0.55\n",
      "testing set, month 170, AUC = 0.59\n",
      "testing set, month 171, accuracy = 0.50\n",
      "testing set, month 171, AUC = 0.52\n",
      "testing set, month 172, accuracy = 0.59\n",
      "testing set, month 172, AUC = 0.65\n",
      "testing set, month 173, accuracy = 0.55\n",
      "testing set, month 173, AUC = 0.58\n",
      "testing set, month 174, accuracy = 0.58\n",
      "testing set, month 174, AUC = 0.62\n",
      "testing set, month 175, accuracy = 0.53\n",
      "testing set, month 175, AUC = 0.54\n",
      "testing set, month 176, accuracy = 0.66\n",
      "testing set, month 176, AUC = 0.70\n",
      "testing set, month 177, accuracy = 0.58\n",
      "testing set, month 177, AUC = 0.61\n",
      "testing set, month 178, accuracy = 0.53\n",
      "testing set, month 178, AUC = 0.53\n",
      "testing set, month 179, accuracy = 0.57\n",
      "testing set, month 179, AUC = 0.61\n",
      "testing set, month 180, accuracy = 0.54\n",
      "testing set, month 180, AUC = 0.56\n",
      "testing set, month 181, accuracy = 0.53\n",
      "testing set, month 181, AUC = 0.55\n",
      "testing set, month 182, accuracy = 0.48\n",
      "testing set, month 182, AUC = 0.47\n",
      "testing set, month 183, accuracy = 0.55\n",
      "testing set, month 183, AUC = 0.57\n",
      "testing set, month 184, accuracy = 0.70\n",
      "testing set, month 184, AUC = 0.75\n",
      "testing set, month 185, accuracy = 0.53\n",
      "testing set, month 185, AUC = 0.54\n",
      "testing set, month 186, accuracy = 0.63\n",
      "testing set, month 186, AUC = 0.67\n",
      "testing set, month 187, accuracy = 0.55\n",
      "testing set, month 187, AUC = 0.56\n",
      "testing set, month 188, accuracy = 0.51\n",
      "testing set, month 188, AUC = 0.52\n",
      "testing set, month 189, accuracy = 0.46\n",
      "testing set, month 189, AUC = 0.44\n",
      "testing set, month 190, accuracy = 0.60\n",
      "testing set, month 190, AUC = 0.65\n",
      "testing set, month 191, accuracy = 0.69\n",
      "testing set, month 191, AUC = 0.75\n",
      "testing set, month 192, accuracy = 0.56\n",
      "testing set, month 192, AUC = 0.57\n",
      "testing set, month 193, accuracy = 0.56\n",
      "testing set, month 193, AUC = 0.58\n",
      "testing set, month 194, accuracy = 0.48\n",
      "testing set, month 194, AUC = 0.48\n",
      "testing set, month 195, accuracy = 0.68\n",
      "testing set, month 195, AUC = 0.74\n",
      "testing set, month 196, accuracy = 0.58\n",
      "testing set, month 196, AUC = 0.61\n",
      "testing set, month 197, accuracy = 0.62\n",
      "testing set, month 197, AUC = 0.67\n",
      "testing set, month 198, accuracy = 0.59\n",
      "testing set, month 198, AUC = 0.62\n",
      "testing set, month 199, accuracy = 0.53\n",
      "testing set, month 199, AUC = 0.55\n",
      "testing set, month 200, accuracy = 0.54\n",
      "testing set, month 200, AUC = 0.54\n",
      "testing set, month 201, accuracy = 0.62\n",
      "testing set, month 201, AUC = 0.66\n",
      "testing set, month 202, accuracy = 0.48\n",
      "testing set, month 202, AUC = 0.48\n",
      "testing set, month 203, accuracy = 0.56\n",
      "testing set, month 203, AUC = 0.58\n",
      "testing set, month 204, accuracy = 0.59\n",
      "testing set, month 204, AUC = 0.62\n",
      "testing set, month 205, accuracy = 0.50\n",
      "testing set, month 205, AUC = 0.52\n",
      "testing set, month 206, accuracy = 0.68\n",
      "testing set, month 206, AUC = 0.74\n",
      "testing set, month 207, accuracy = 0.62\n",
      "testing set, month 207, AUC = 0.66\n",
      "testing set, month 208, accuracy = 0.60\n",
      "testing set, month 208, AUC = 0.64\n",
      "testing set, month 209, accuracy = 0.58\n",
      "testing set, month 209, AUC = 0.63\n",
      "testing set, month 210, accuracy = 0.56\n",
      "testing set, month 210, AUC = 0.57\n",
      "testing set, month 211, accuracy = 0.54\n",
      "testing set, month 211, AUC = 0.56\n",
      "testing set, month 212, accuracy = 0.64\n",
      "testing set, month 212, AUC = 0.71\n",
      "testing set, month 213, accuracy = 0.62\n",
      "testing set, month 213, AUC = 0.66\n",
      "testing set, month 214, accuracy = 0.56\n",
      "testing set, month 214, AUC = 0.60\n",
      "testing set, month 215, accuracy = 0.61\n",
      "testing set, month 215, AUC = 0.65\n",
      "testing set, month 216, accuracy = 0.59\n",
      "testing set, month 216, AUC = 0.65\n",
      "testing set, month 217, accuracy = 0.53\n",
      "testing set, month 217, AUC = 0.54\n",
      "testing set, month 218, accuracy = 0.53\n",
      "testing set, month 218, AUC = 0.54\n",
      "testing set, month 219, accuracy = 0.71\n",
      "testing set, month 219, AUC = 0.76\n",
      "testing set, month 220, accuracy = 0.57\n",
      "testing set, month 220, AUC = 0.60\n",
      "testing set, month 221, accuracy = 0.60\n",
      "testing set, month 221, AUC = 0.63\n",
      "testing set, month 222, accuracy = 0.57\n",
      "testing set, month 222, AUC = 0.58\n",
      "testing set, month 223, accuracy = 0.57\n",
      "testing set, month 223, AUC = 0.61\n",
      "testing set, month 224, accuracy = 0.60\n",
      "testing set, month 224, AUC = 0.65\n",
      "testing set, month 225, accuracy = 0.51\n",
      "testing set, month 225, AUC = 0.53\n",
      "testing set, month 226, accuracy = 0.54\n",
      "testing set, month 226, AUC = 0.55\n",
      "testing set, month 227, accuracy = 0.54\n",
      "testing set, month 227, AUC = 0.53\n",
      "testing set, month 228, accuracy = 0.48\n",
      "testing set, month 228, AUC = 0.50\n",
      "testing set, month 229, accuracy = 0.48\n",
      "testing set, month 229, AUC = 0.49\n",
      "testing set, month 230, accuracy = 0.64\n",
      "testing set, month 230, AUC = 0.69\n",
      "testing set, month 231, accuracy = 0.56\n",
      "testing set, month 231, AUC = 0.59\n",
      "testing set, month 232, accuracy = 0.58\n",
      "testing set, month 232, AUC = 0.62\n",
      "testing set, month 233, accuracy = 0.52\n",
      "testing set, month 233, AUC = 0.55\n",
      "testing set, month 234, accuracy = 0.56\n",
      "testing set, month 234, AUC = 0.60\n",
      "testing set, month 235, accuracy = 0.53\n",
      "testing set, month 235, AUC = 0.55\n",
      "testing set, month 236, accuracy = 0.48\n",
      "testing set, month 236, AUC = 0.50\n",
      "testing set, month 237, accuracy = 0.54\n",
      "testing set, month 237, AUC = 0.57\n",
      "testing set, month 238, accuracy = 0.55\n",
      "testing set, month 238, AUC = 0.59\n",
      "testing set, month 239, accuracy = 0.51\n",
      "testing set, month 239, AUC = 0.53\n",
      "testing set, month 240, accuracy = 0.54\n",
      "testing set, month 240, AUC = 0.57\n",
      "testing set, month 241, accuracy = 0.55\n",
      "testing set, month 241, AUC = 0.59\n",
      "testing set, month 242, accuracy = 0.50\n",
      "testing set, month 242, AUC = 0.51\n",
      "testing set, month 243, accuracy = 0.64\n",
      "testing set, month 243, AUC = 0.69\n",
      "testing set, month 244, accuracy = 0.50\n",
      "testing set, month 244, AUC = 0.51\n",
      "testing set, month 245, accuracy = 0.53\n",
      "testing set, month 245, AUC = 0.55\n",
      "testing set, month 246, accuracy = 0.54\n",
      "testing set, month 246, AUC = 0.55\n",
      "testing set, month 247, accuracy = 0.64\n",
      "testing set, month 247, AUC = 0.69\n",
      "testing set, month 248, accuracy = 0.55\n",
      "testing set, month 248, AUC = 0.56\n",
      "testing set, month 249, accuracy = 0.56\n",
      "testing set, month 249, AUC = 0.59\n",
      "testing set, month 250, accuracy = 0.59\n",
      "testing set, month 250, AUC = 0.63\n",
      "testing set, month 251, accuracy = 0.57\n",
      "testing set, month 251, AUC = 0.59\n",
      "testing set, month 252, accuracy = 0.63\n",
      "testing set, month 252, AUC = 0.68\n",
      "testing set, month 253, accuracy = 0.55\n",
      "testing set, month 253, AUC = 0.59\n",
      "testing set, month 254, accuracy = 0.61\n",
      "testing set, month 254, AUC = 0.64\n",
      "testing set, month 255, accuracy = 0.56\n",
      "testing set, month 255, AUC = 0.58\n",
      "testing set, month 256, accuracy = 0.47\n",
      "testing set, month 256, AUC = 0.46\n",
      "testing set, month 257, accuracy = 0.55\n",
      "testing set, month 257, AUC = 0.57\n",
      "testing set, month 258, accuracy = 0.56\n",
      "testing set, month 258, AUC = 0.60\n",
      "testing set, month 259, accuracy = 0.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing set, month 259, AUC = 0.56\n",
      "testing set, month 260, accuracy = 0.53\n",
      "testing set, month 260, AUC = 0.55\n",
      "testing set, month 261, accuracy = 0.50\n",
      "testing set, month 261, AUC = 0.50\n",
      "testing set, month 262, accuracy = 0.48\n",
      "testing set, month 262, AUC = 0.47\n",
      "testing set, month 263, accuracy = 0.66\n",
      "testing set, month 263, AUC = 0.72\n",
      "testing set, month 264, accuracy = 0.50\n",
      "testing set, month 264, AUC = 0.50\n",
      "testing set, month 265, accuracy = 0.48\n",
      "testing set, month 265, AUC = 0.49\n",
      "testing set, month 266, accuracy = 0.50\n",
      "testing set, month 266, AUC = 0.51\n",
      "testing set, month 267, accuracy = 0.56\n",
      "testing set, month 267, AUC = 0.59\n",
      "testing set, month 268, accuracy = 0.62\n",
      "testing set, month 268, AUC = 0.67\n",
      "testing set, month 269, accuracy = 0.53\n",
      "testing set, month 269, AUC = 0.55\n",
      "##----------------------------------\n",
      "4.SIMPLE STRATEGY:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annual excess return = 0.29\n",
      "annual excess volatility = 0.25\n",
      "information ratio = 1.18\n",
      "\n",
      "training times = 2\n",
      "##----------------------------------\n",
      "1.CROSS-VALIDATION:\n",
      "[0.58173332 0.59149252 0.58714331 0.58258195 0.58328029]\n",
      "##----------------------------------\n",
      "2.INTERNAL DEPENDENCE:\n",
      "Mean Squared Error(MSE) = 0.237957\n",
      "Median Absolute Error(MAE) = 0.479588\n",
      "R^2 score = 0.048162\n",
      "##----------------------------------\n",
      "3.MODOLE EVALUATION:\n",
      "training set, accuracy = 0.61\n",
      "training set, AUC = 0.66\n",
      "cv set, accuracy = 0.58\n",
      "cv set, AUC = 0.62\n",
      "testing set, month 145, accuracy = 0.52\n",
      "testing set, month 145, AUC = 0.53\n",
      "testing set, month 146, accuracy = 0.56\n",
      "testing set, month 146, AUC = 0.61\n",
      "testing set, month 147, accuracy = 0.68\n",
      "testing set, month 147, AUC = 0.76\n",
      "testing set, month 148, accuracy = 0.55\n",
      "testing set, month 148, AUC = 0.59\n",
      "testing set, month 149, accuracy = 0.61\n",
      "testing set, month 149, AUC = 0.65\n",
      "testing set, month 150, accuracy = 0.55\n",
      "testing set, month 150, AUC = 0.53\n",
      "testing set, month 151, accuracy = 0.57\n",
      "testing set, month 151, AUC = 0.61\n",
      "testing set, month 152, accuracy = 0.53\n",
      "testing set, month 152, AUC = 0.55\n",
      "testing set, month 153, accuracy = 0.64\n",
      "testing set, month 153, AUC = 0.69\n",
      "testing set, month 154, accuracy = 0.54\n",
      "testing set, month 154, AUC = 0.53\n",
      "testing set, month 155, accuracy = 0.61\n",
      "testing set, month 155, AUC = 0.65\n",
      "testing set, month 156, accuracy = 0.53\n",
      "testing set, month 156, AUC = 0.55\n",
      "testing set, month 157, accuracy = 0.55\n",
      "testing set, month 157, AUC = 0.59\n",
      "testing set, month 158, accuracy = 0.56\n",
      "testing set, month 158, AUC = 0.57\n",
      "testing set, month 159, accuracy = 0.54\n",
      "testing set, month 159, AUC = 0.58\n",
      "testing set, month 160, accuracy = 0.54\n",
      "testing set, month 160, AUC = 0.57\n",
      "testing set, month 161, accuracy = 0.60\n",
      "testing set, month 161, AUC = 0.64\n",
      "testing set, month 162, accuracy = 0.55\n",
      "testing set, month 162, AUC = 0.57\n",
      "testing set, month 163, accuracy = 0.57\n",
      "testing set, month 163, AUC = 0.59\n",
      "testing set, month 164, accuracy = 0.51\n",
      "testing set, month 164, AUC = 0.51\n",
      "testing set, month 165, accuracy = 0.61\n",
      "testing set, month 165, AUC = 0.65\n",
      "testing set, month 166, accuracy = 0.58\n",
      "testing set, month 166, AUC = 0.60\n",
      "testing set, month 167, accuracy = 0.60\n",
      "testing set, month 167, AUC = 0.64\n",
      "testing set, month 168, accuracy = 0.57\n",
      "testing set, month 168, AUC = 0.59\n",
      "testing set, month 169, accuracy = 0.56\n",
      "testing set, month 169, AUC = 0.60\n",
      "testing set, month 170, accuracy = 0.55\n",
      "testing set, month 170, AUC = 0.60\n",
      "testing set, month 171, accuracy = 0.51\n",
      "testing set, month 171, AUC = 0.53\n",
      "testing set, month 172, accuracy = 0.60\n",
      "testing set, month 172, AUC = 0.64\n",
      "testing set, month 173, accuracy = 0.57\n",
      "testing set, month 173, AUC = 0.60\n",
      "testing set, month 174, accuracy = 0.58\n",
      "testing set, month 174, AUC = 0.63\n",
      "testing set, month 175, accuracy = 0.53\n",
      "testing set, month 175, AUC = 0.53\n",
      "testing set, month 176, accuracy = 0.65\n",
      "testing set, month 176, AUC = 0.69\n",
      "testing set, month 177, accuracy = 0.60\n",
      "testing set, month 177, AUC = 0.63\n",
      "testing set, month 178, accuracy = 0.52\n",
      "testing set, month 178, AUC = 0.54\n",
      "testing set, month 179, accuracy = 0.58\n",
      "testing set, month 179, AUC = 0.62\n",
      "testing set, month 180, accuracy = 0.53\n",
      "testing set, month 180, AUC = 0.56\n",
      "testing set, month 181, accuracy = 0.54\n",
      "testing set, month 181, AUC = 0.56\n",
      "testing set, month 182, accuracy = 0.47\n",
      "testing set, month 182, AUC = 0.47\n",
      "testing set, month 183, accuracy = 0.55\n",
      "testing set, month 183, AUC = 0.57\n",
      "testing set, month 184, accuracy = 0.70\n",
      "testing set, month 184, AUC = 0.75\n",
      "testing set, month 185, accuracy = 0.52\n",
      "testing set, month 185, AUC = 0.55\n",
      "testing set, month 186, accuracy = 0.63\n",
      "testing set, month 186, AUC = 0.67\n",
      "testing set, month 187, accuracy = 0.54\n",
      "testing set, month 187, AUC = 0.55\n",
      "testing set, month 188, accuracy = 0.52\n",
      "testing set, month 188, AUC = 0.54\n",
      "testing set, month 189, accuracy = 0.47\n",
      "testing set, month 189, AUC = 0.46\n",
      "testing set, month 190, accuracy = 0.61\n",
      "testing set, month 190, AUC = 0.66\n",
      "testing set, month 191, accuracy = 0.69\n",
      "testing set, month 191, AUC = 0.74\n",
      "testing set, month 192, accuracy = 0.55\n",
      "testing set, month 192, AUC = 0.58\n",
      "testing set, month 193, accuracy = 0.56\n",
      "testing set, month 193, AUC = 0.59\n",
      "testing set, month 194, accuracy = 0.48\n",
      "testing set, month 194, AUC = 0.50\n",
      "testing set, month 195, accuracy = 0.68\n",
      "testing set, month 195, AUC = 0.73\n",
      "testing set, month 196, accuracy = 0.59\n",
      "testing set, month 196, AUC = 0.62\n",
      "testing set, month 197, accuracy = 0.60\n",
      "testing set, month 197, AUC = 0.66\n",
      "testing set, month 198, accuracy = 0.60\n",
      "testing set, month 198, AUC = 0.62\n",
      "testing set, month 199, accuracy = 0.52\n",
      "testing set, month 199, AUC = 0.54\n",
      "testing set, month 200, accuracy = 0.52\n",
      "testing set, month 200, AUC = 0.52\n",
      "testing set, month 201, accuracy = 0.63\n",
      "testing set, month 201, AUC = 0.68\n",
      "testing set, month 202, accuracy = 0.48\n",
      "testing set, month 202, AUC = 0.49\n",
      "testing set, month 203, accuracy = 0.56\n",
      "testing set, month 203, AUC = 0.59\n",
      "testing set, month 204, accuracy = 0.58\n",
      "testing set, month 204, AUC = 0.61\n",
      "testing set, month 205, accuracy = 0.52\n",
      "testing set, month 205, AUC = 0.55\n",
      "testing set, month 206, accuracy = 0.66\n",
      "testing set, month 206, AUC = 0.72\n",
      "testing set, month 207, accuracy = 0.62\n",
      "testing set, month 207, AUC = 0.65\n",
      "testing set, month 208, accuracy = 0.59\n",
      "testing set, month 208, AUC = 0.64\n",
      "testing set, month 209, accuracy = 0.59\n",
      "testing set, month 209, AUC = 0.65\n",
      "testing set, month 210, accuracy = 0.56\n",
      "testing set, month 210, AUC = 0.58\n",
      "testing set, month 211, accuracy = 0.54\n",
      "testing set, month 211, AUC = 0.58\n",
      "testing set, month 212, accuracy = 0.66\n",
      "testing set, month 212, AUC = 0.71\n",
      "testing set, month 213, accuracy = 0.62\n",
      "testing set, month 213, AUC = 0.66\n",
      "testing set, month 214, accuracy = 0.57\n",
      "testing set, month 214, AUC = 0.60\n",
      "testing set, month 215, accuracy = 0.61\n",
      "testing set, month 215, AUC = 0.66\n",
      "testing set, month 216, accuracy = 0.60\n",
      "testing set, month 216, AUC = 0.66\n",
      "testing set, month 217, accuracy = 0.54\n",
      "testing set, month 217, AUC = 0.54\n",
      "testing set, month 218, accuracy = 0.52\n",
      "testing set, month 218, AUC = 0.55\n",
      "testing set, month 219, accuracy = 0.70\n",
      "testing set, month 219, AUC = 0.75\n",
      "testing set, month 220, accuracy = 0.59\n",
      "testing set, month 220, AUC = 0.61\n",
      "testing set, month 221, accuracy = 0.61\n",
      "testing set, month 221, AUC = 0.64\n",
      "testing set, month 222, accuracy = 0.56\n",
      "testing set, month 222, AUC = 0.58\n",
      "testing set, month 223, accuracy = 0.57\n",
      "testing set, month 223, AUC = 0.60\n",
      "testing set, month 224, accuracy = 0.60\n",
      "testing set, month 224, AUC = 0.65\n",
      "testing set, month 225, accuracy = 0.52\n",
      "testing set, month 225, AUC = 0.53\n",
      "testing set, month 226, accuracy = 0.55\n",
      "testing set, month 226, AUC = 0.57\n",
      "testing set, month 227, accuracy = 0.54\n",
      "testing set, month 227, AUC = 0.53\n",
      "testing set, month 228, accuracy = 0.50\n",
      "testing set, month 228, AUC = 0.50\n",
      "testing set, month 229, accuracy = 0.49\n",
      "testing set, month 229, AUC = 0.49\n",
      "testing set, month 230, accuracy = 0.65\n",
      "testing set, month 230, AUC = 0.70\n",
      "testing set, month 231, accuracy = 0.56\n",
      "testing set, month 231, AUC = 0.57\n",
      "testing set, month 232, accuracy = 0.60\n",
      "testing set, month 232, AUC = 0.63\n",
      "testing set, month 233, accuracy = 0.54\n",
      "testing set, month 233, AUC = 0.56\n",
      "testing set, month 234, accuracy = 0.57\n",
      "testing set, month 234, AUC = 0.60\n",
      "testing set, month 235, accuracy = 0.53\n",
      "testing set, month 235, AUC = 0.55\n",
      "testing set, month 236, accuracy = 0.48\n",
      "testing set, month 236, AUC = 0.50\n",
      "testing set, month 237, accuracy = 0.55\n",
      "testing set, month 237, AUC = 0.57\n",
      "testing set, month 238, accuracy = 0.56\n",
      "testing set, month 238, AUC = 0.59\n",
      "testing set, month 239, accuracy = 0.52\n",
      "testing set, month 239, AUC = 0.54\n",
      "testing set, month 240, accuracy = 0.55\n",
      "testing set, month 240, AUC = 0.57\n",
      "testing set, month 241, accuracy = 0.56\n",
      "testing set, month 241, AUC = 0.59\n",
      "testing set, month 242, accuracy = 0.48\n",
      "testing set, month 242, AUC = 0.49\n",
      "testing set, month 243, accuracy = 0.65\n",
      "testing set, month 243, AUC = 0.70\n",
      "testing set, month 244, accuracy = 0.51\n",
      "testing set, month 244, AUC = 0.53\n",
      "testing set, month 245, accuracy = 0.52\n",
      "testing set, month 245, AUC = 0.54\n",
      "testing set, month 246, accuracy = 0.54\n",
      "testing set, month 246, AUC = 0.56\n",
      "testing set, month 247, accuracy = 0.64\n",
      "testing set, month 247, AUC = 0.70\n",
      "testing set, month 248, accuracy = 0.55\n",
      "testing set, month 248, AUC = 0.56\n",
      "testing set, month 249, accuracy = 0.55\n",
      "testing set, month 249, AUC = 0.59\n",
      "testing set, month 250, accuracy = 0.60\n",
      "testing set, month 250, AUC = 0.63\n",
      "testing set, month 251, accuracy = 0.58\n",
      "testing set, month 251, AUC = 0.60\n",
      "testing set, month 252, accuracy = 0.63\n",
      "testing set, month 252, AUC = 0.68\n",
      "testing set, month 253, accuracy = 0.56\n",
      "testing set, month 253, AUC = 0.59\n",
      "testing set, month 254, accuracy = 0.60\n",
      "testing set, month 254, AUC = 0.64\n",
      "testing set, month 255, accuracy = 0.57\n",
      "testing set, month 255, AUC = 0.59\n",
      "testing set, month 256, accuracy = 0.47\n",
      "testing set, month 256, AUC = 0.46\n",
      "testing set, month 257, accuracy = 0.56\n",
      "testing set, month 257, AUC = 0.58\n",
      "testing set, month 258, accuracy = 0.56\n",
      "testing set, month 258, AUC = 0.60\n",
      "testing set, month 259, accuracy = 0.54\n",
      "testing set, month 259, AUC = 0.56\n",
      "testing set, month 260, accuracy = 0.53\n",
      "testing set, month 260, AUC = 0.55\n",
      "testing set, month 261, accuracy = 0.50\n",
      "testing set, month 261, AUC = 0.50\n",
      "testing set, month 262, accuracy = 0.48\n",
      "testing set, month 262, AUC = 0.47\n",
      "testing set, month 263, accuracy = 0.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing set, month 263, AUC = 0.72\n",
      "testing set, month 264, accuracy = 0.50\n",
      "testing set, month 264, AUC = 0.49\n",
      "testing set, month 265, accuracy = 0.49\n",
      "testing set, month 265, AUC = 0.50\n",
      "testing set, month 266, accuracy = 0.50\n",
      "testing set, month 266, AUC = 0.50\n",
      "testing set, month 267, accuracy = 0.57\n",
      "testing set, month 267, AUC = 0.60\n",
      "testing set, month 268, accuracy = 0.62\n",
      "testing set, month 268, AUC = 0.67\n",
      "testing set, month 269, accuracy = 0.54\n",
      "testing set, month 269, AUC = 0.55\n",
      "##----------------------------------\n",
      "4.SIMPLE STRATEGY:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annual excess return = 0.30\n",
      "annual excess volatility = 0.26\n",
      "information ratio = 1.17\n",
      "\n",
      "training times = 3\n",
      "##----------------------------------\n",
      "1.CROSS-VALIDATION:\n",
      "[0.58739837 0.58252033 0.57729444 0.58664498 0.58125826]\n",
      "##----------------------------------\n",
      "2.INTERNAL DEPENDENCE:\n",
      "Mean Squared Error(MSE) = 0.237326\n",
      "Median Absolute Error(MAE) = 0.478151\n",
      "R^2 score = 0.050422\n",
      "##----------------------------------\n",
      "3.MODOLE EVALUATION:\n",
      "training set, accuracy = 0.61\n",
      "training set, AUC = 0.66\n",
      "cv set, accuracy = 0.59\n",
      "cv set, AUC = 0.62\n",
      "testing set, month 145, accuracy = 0.48\n",
      "testing set, month 145, AUC = 0.48\n",
      "testing set, month 146, accuracy = 0.55\n",
      "testing set, month 146, AUC = 0.58\n",
      "testing set, month 147, accuracy = 0.70\n",
      "testing set, month 147, AUC = 0.77\n",
      "testing set, month 148, accuracy = 0.56\n",
      "testing set, month 148, AUC = 0.59\n",
      "testing set, month 149, accuracy = 0.62\n",
      "testing set, month 149, AUC = 0.66\n",
      "testing set, month 150, accuracy = 0.52\n",
      "testing set, month 150, AUC = 0.51\n",
      "testing set, month 151, accuracy = 0.56\n",
      "testing set, month 151, AUC = 0.62\n",
      "testing set, month 152, accuracy = 0.55\n",
      "testing set, month 152, AUC = 0.58\n",
      "testing set, month 153, accuracy = 0.72\n",
      "testing set, month 153, AUC = 0.77\n",
      "testing set, month 154, accuracy = 0.59\n",
      "testing set, month 154, AUC = 0.61\n",
      "testing set, month 155, accuracy = 0.66\n",
      "testing set, month 155, AUC = 0.73\n",
      "testing set, month 156, accuracy = 0.62\n",
      "testing set, month 156, AUC = 0.67\n",
      "testing set, month 157, accuracy = 0.54\n",
      "testing set, month 157, AUC = 0.59\n",
      "testing set, month 158, accuracy = 0.56\n",
      "testing set, month 158, AUC = 0.56\n",
      "testing set, month 159, accuracy = 0.51\n",
      "testing set, month 159, AUC = 0.54\n",
      "testing set, month 160, accuracy = 0.51\n",
      "testing set, month 160, AUC = 0.54\n",
      "testing set, month 161, accuracy = 0.60\n",
      "testing set, month 161, AUC = 0.65\n",
      "testing set, month 162, accuracy = 0.53\n",
      "testing set, month 162, AUC = 0.54\n",
      "testing set, month 163, accuracy = 0.55\n",
      "testing set, month 163, AUC = 0.57\n",
      "testing set, month 164, accuracy = 0.49\n",
      "testing set, month 164, AUC = 0.49\n",
      "testing set, month 165, accuracy = 0.65\n",
      "testing set, month 165, AUC = 0.70\n",
      "testing set, month 166, accuracy = 0.54\n",
      "testing set, month 166, AUC = 0.57\n",
      "testing set, month 167, accuracy = 0.61\n",
      "testing set, month 167, AUC = 0.64\n",
      "testing set, month 168, accuracy = 0.59\n",
      "testing set, month 168, AUC = 0.63\n",
      "testing set, month 169, accuracy = 0.55\n",
      "testing set, month 169, AUC = 0.58\n",
      "testing set, month 170, accuracy = 0.53\n",
      "testing set, month 170, AUC = 0.56\n",
      "testing set, month 171, accuracy = 0.51\n",
      "testing set, month 171, AUC = 0.52\n",
      "testing set, month 172, accuracy = 0.58\n",
      "testing set, month 172, AUC = 0.61\n",
      "testing set, month 173, accuracy = 0.56\n",
      "testing set, month 173, AUC = 0.60\n",
      "testing set, month 174, accuracy = 0.59\n",
      "testing set, month 174, AUC = 0.65\n",
      "testing set, month 175, accuracy = 0.54\n",
      "testing set, month 175, AUC = 0.56\n",
      "testing set, month 176, accuracy = 0.61\n",
      "testing set, month 176, AUC = 0.64\n",
      "testing set, month 177, accuracy = 0.59\n",
      "testing set, month 177, AUC = 0.62\n",
      "testing set, month 178, accuracy = 0.50\n",
      "testing set, month 178, AUC = 0.50\n",
      "testing set, month 179, accuracy = 0.57\n",
      "testing set, month 179, AUC = 0.62\n",
      "testing set, month 180, accuracy = 0.52\n",
      "testing set, month 180, AUC = 0.54\n",
      "testing set, month 181, accuracy = 0.53\n",
      "testing set, month 181, AUC = 0.54\n",
      "testing set, month 182, accuracy = 0.44\n",
      "testing set, month 182, AUC = 0.44\n",
      "testing set, month 183, accuracy = 0.53\n",
      "testing set, month 183, AUC = 0.54\n",
      "testing set, month 184, accuracy = 0.72\n",
      "testing set, month 184, AUC = 0.76\n",
      "testing set, month 185, accuracy = 0.53\n",
      "testing set, month 185, AUC = 0.53\n",
      "testing set, month 186, accuracy = 0.63\n",
      "testing set, month 186, AUC = 0.68\n",
      "testing set, month 187, accuracy = 0.53\n",
      "testing set, month 187, AUC = 0.52\n",
      "testing set, month 188, accuracy = 0.50\n",
      "testing set, month 188, AUC = 0.52\n",
      "testing set, month 189, accuracy = 0.44\n",
      "testing set, month 189, AUC = 0.42\n",
      "testing set, month 190, accuracy = 0.62\n",
      "testing set, month 190, AUC = 0.67\n",
      "testing set, month 191, accuracy = 0.72\n",
      "testing set, month 191, AUC = 0.78\n",
      "testing set, month 192, accuracy = 0.58\n",
      "testing set, month 192, AUC = 0.59\n",
      "testing set, month 193, accuracy = 0.53\n",
      "testing set, month 193, AUC = 0.55\n",
      "testing set, month 194, accuracy = 0.47\n",
      "testing set, month 194, AUC = 0.48\n",
      "testing set, month 195, accuracy = 0.69\n",
      "testing set, month 195, AUC = 0.75\n",
      "testing set, month 196, accuracy = 0.58\n",
      "testing set, month 196, AUC = 0.61\n",
      "testing set, month 197, accuracy = 0.60\n",
      "testing set, month 197, AUC = 0.67\n",
      "testing set, month 198, accuracy = 0.59\n",
      "testing set, month 198, AUC = 0.61\n",
      "testing set, month 199, accuracy = 0.54\n",
      "testing set, month 199, AUC = 0.55\n",
      "testing set, month 200, accuracy = 0.54\n",
      "testing set, month 200, AUC = 0.55\n",
      "testing set, month 201, accuracy = 0.59\n",
      "testing set, month 201, AUC = 0.63\n",
      "testing set, month 202, accuracy = 0.48\n",
      "testing set, month 202, AUC = 0.48\n",
      "testing set, month 203, accuracy = 0.57\n",
      "testing set, month 203, AUC = 0.59\n",
      "testing set, month 204, accuracy = 0.59\n",
      "testing set, month 204, AUC = 0.62\n",
      "testing set, month 205, accuracy = 0.49\n",
      "testing set, month 205, AUC = 0.51\n",
      "testing set, month 206, accuracy = 0.68\n",
      "testing set, month 206, AUC = 0.73\n",
      "testing set, month 207, accuracy = 0.60\n",
      "testing set, month 207, AUC = 0.63\n",
      "testing set, month 208, accuracy = 0.62\n",
      "testing set, month 208, AUC = 0.66\n",
      "testing set, month 209, accuracy = 0.57\n",
      "testing set, month 209, AUC = 0.60\n",
      "testing set, month 210, accuracy = 0.56\n",
      "testing set, month 210, AUC = 0.57\n",
      "testing set, month 211, accuracy = 0.53\n",
      "testing set, month 211, AUC = 0.56\n",
      "testing set, month 212, accuracy = 0.66\n",
      "testing set, month 212, AUC = 0.71\n",
      "testing set, month 213, accuracy = 0.61\n",
      "testing set, month 213, AUC = 0.65\n",
      "testing set, month 214, accuracy = 0.59\n",
      "testing set, month 214, AUC = 0.62\n",
      "testing set, month 215, accuracy = 0.59\n",
      "testing set, month 215, AUC = 0.63\n",
      "testing set, month 216, accuracy = 0.60\n",
      "testing set, month 216, AUC = 0.65\n",
      "testing set, month 217, accuracy = 0.52\n",
      "testing set, month 217, AUC = 0.52\n",
      "testing set, month 218, accuracy = 0.51\n",
      "testing set, month 218, AUC = 0.53\n",
      "testing set, month 219, accuracy = 0.71\n",
      "testing set, month 219, AUC = 0.76\n",
      "testing set, month 220, accuracy = 0.59\n",
      "testing set, month 220, AUC = 0.61\n",
      "testing set, month 221, accuracy = 0.59\n",
      "testing set, month 221, AUC = 0.64\n",
      "testing set, month 222, accuracy = 0.54\n",
      "testing set, month 222, AUC = 0.56\n",
      "testing set, month 223, accuracy = 0.56\n",
      "testing set, month 223, AUC = 0.60\n",
      "testing set, month 224, accuracy = 0.62\n",
      "testing set, month 224, AUC = 0.68\n",
      "testing set, month 225, accuracy = 0.57\n",
      "testing set, month 225, AUC = 0.60\n",
      "testing set, month 226, accuracy = 0.53\n",
      "testing set, month 226, AUC = 0.55\n",
      "testing set, month 227, accuracy = 0.53\n",
      "testing set, month 227, AUC = 0.51\n",
      "testing set, month 228, accuracy = 0.51\n",
      "testing set, month 228, AUC = 0.52\n",
      "testing set, month 229, accuracy = 0.51\n",
      "testing set, month 229, AUC = 0.52\n",
      "testing set, month 230, accuracy = 0.63\n",
      "testing set, month 230, AUC = 0.68\n",
      "testing set, month 231, accuracy = 0.61\n",
      "testing set, month 231, AUC = 0.64\n",
      "testing set, month 232, accuracy = 0.55\n",
      "testing set, month 232, AUC = 0.58\n",
      "testing set, month 233, accuracy = 0.53\n",
      "testing set, month 233, AUC = 0.54\n",
      "testing set, month 234, accuracy = 0.54\n",
      "testing set, month 234, AUC = 0.58\n",
      "testing set, month 235, accuracy = 0.54\n",
      "testing set, month 235, AUC = 0.57\n",
      "testing set, month 236, accuracy = 0.49\n",
      "testing set, month 236, AUC = 0.51\n",
      "testing set, month 237, accuracy = 0.55\n",
      "testing set, month 237, AUC = 0.58\n",
      "testing set, month 238, accuracy = 0.54\n",
      "testing set, month 238, AUC = 0.57\n",
      "testing set, month 239, accuracy = 0.48\n",
      "testing set, month 239, AUC = 0.49\n",
      "testing set, month 240, accuracy = 0.55\n",
      "testing set, month 240, AUC = 0.57\n",
      "testing set, month 241, accuracy = 0.54\n",
      "testing set, month 241, AUC = 0.56\n",
      "testing set, month 242, accuracy = 0.47\n",
      "testing set, month 242, AUC = 0.47\n",
      "testing set, month 243, accuracy = 0.68\n",
      "testing set, month 243, AUC = 0.72\n",
      "testing set, month 244, accuracy = 0.55\n",
      "testing set, month 244, AUC = 0.57\n",
      "testing set, month 245, accuracy = 0.55\n",
      "testing set, month 245, AUC = 0.58\n",
      "testing set, month 246, accuracy = 0.52\n",
      "testing set, month 246, AUC = 0.53\n",
      "testing set, month 247, accuracy = 0.65\n",
      "testing set, month 247, AUC = 0.70\n",
      "testing set, month 248, accuracy = 0.53\n",
      "testing set, month 248, AUC = 0.54\n",
      "testing set, month 249, accuracy = 0.55\n",
      "testing set, month 249, AUC = 0.61\n",
      "testing set, month 250, accuracy = 0.58\n",
      "testing set, month 250, AUC = 0.62\n",
      "testing set, month 251, accuracy = 0.56\n",
      "testing set, month 251, AUC = 0.59\n",
      "testing set, month 252, accuracy = 0.64\n",
      "testing set, month 252, AUC = 0.69\n",
      "testing set, month 253, accuracy = 0.54\n",
      "testing set, month 253, AUC = 0.56\n",
      "testing set, month 254, accuracy = 0.58\n",
      "testing set, month 254, AUC = 0.62\n",
      "testing set, month 255, accuracy = 0.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing set, month 255, AUC = 0.58\n",
      "testing set, month 256, accuracy = 0.45\n",
      "testing set, month 256, AUC = 0.44\n",
      "testing set, month 257, accuracy = 0.54\n",
      "testing set, month 257, AUC = 0.56\n",
      "testing set, month 258, accuracy = 0.57\n",
      "testing set, month 258, AUC = 0.60\n",
      "testing set, month 259, accuracy = 0.56\n",
      "testing set, month 259, AUC = 0.57\n",
      "testing set, month 260, accuracy = 0.52\n",
      "testing set, month 260, AUC = 0.53\n",
      "testing set, month 261, accuracy = 0.48\n",
      "testing set, month 261, AUC = 0.49\n",
      "testing set, month 262, accuracy = 0.47\n",
      "testing set, month 262, AUC = 0.46\n",
      "testing set, month 263, accuracy = 0.68\n",
      "testing set, month 263, AUC = 0.74\n",
      "testing set, month 264, accuracy = 0.46\n",
      "testing set, month 264, AUC = 0.46\n",
      "testing set, month 265, accuracy = 0.48\n",
      "testing set, month 265, AUC = 0.48\n",
      "testing set, month 266, accuracy = 0.48\n",
      "testing set, month 266, AUC = 0.48\n",
      "testing set, month 267, accuracy = 0.57\n",
      "testing set, month 267, AUC = 0.60\n",
      "testing set, month 268, accuracy = 0.62\n",
      "testing set, month 268, AUC = 0.67\n",
      "testing set, month 269, accuracy = 0.53\n",
      "testing set, month 269, AUC = 0.55\n",
      "##----------------------------------\n",
      "4.SIMPLE STRATEGY:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annual excess return = 0.30\n",
      "annual excess volatility = 0.25\n",
      "information ratio = 1.19\n",
      "\n",
      "training times = 4\n",
      "##----------------------------------\n",
      "1.CROSS-VALIDATION:\n",
      "[0.5835808  0.58147332 0.57141489 0.58080276 0.58219966]\n",
      "##----------------------------------\n",
      "2.INTERNAL DEPENDENCE:\n",
      "Mean Squared Error(MSE) = 0.239671\n",
      "Median Absolute Error(MAE) = 0.481484\n",
      "R^2 score = 0.040891\n",
      "##----------------------------------\n",
      "3.MODOLE EVALUATION:\n",
      "training set, accuracy = 0.61\n",
      "training set, AUC = 0.65\n",
      "cv set, accuracy = 0.58\n",
      "cv set, AUC = 0.61\n",
      "testing set, month 145, accuracy = 0.50\n",
      "testing set, month 145, AUC = 0.50\n",
      "testing set, month 146, accuracy = 0.57\n",
      "testing set, month 146, AUC = 0.61\n",
      "testing set, month 147, accuracy = 0.70\n",
      "testing set, month 147, AUC = 0.76\n",
      "testing set, month 148, accuracy = 0.54\n",
      "testing set, month 148, AUC = 0.59\n",
      "testing set, month 149, accuracy = 0.60\n",
      "testing set, month 149, AUC = 0.65\n",
      "testing set, month 150, accuracy = 0.52\n",
      "testing set, month 150, AUC = 0.51\n",
      "testing set, month 151, accuracy = 0.59\n",
      "testing set, month 151, AUC = 0.64\n",
      "testing set, month 152, accuracy = 0.56\n",
      "testing set, month 152, AUC = 0.58\n",
      "testing set, month 153, accuracy = 0.69\n",
      "testing set, month 153, AUC = 0.75\n",
      "testing set, month 154, accuracy = 0.55\n",
      "testing set, month 154, AUC = 0.58\n",
      "testing set, month 155, accuracy = 0.64\n",
      "testing set, month 155, AUC = 0.70\n",
      "testing set, month 156, accuracy = 0.58\n",
      "testing set, month 156, AUC = 0.63\n",
      "testing set, month 157, accuracy = 0.58\n",
      "testing set, month 157, AUC = 0.64\n",
      "testing set, month 158, accuracy = 0.58\n",
      "testing set, month 158, AUC = 0.60\n",
      "testing set, month 159, accuracy = 0.57\n",
      "testing set, month 159, AUC = 0.62\n",
      "testing set, month 160, accuracy = 0.58\n",
      "testing set, month 160, AUC = 0.61\n",
      "testing set, month 161, accuracy = 0.63\n",
      "testing set, month 161, AUC = 0.68\n",
      "testing set, month 162, accuracy = 0.57\n",
      "testing set, month 162, AUC = 0.62\n",
      "testing set, month 163, accuracy = 0.57\n",
      "testing set, month 163, AUC = 0.60\n",
      "testing set, month 164, accuracy = 0.50\n",
      "testing set, month 164, AUC = 0.51\n",
      "testing set, month 165, accuracy = 0.62\n",
      "testing set, month 165, AUC = 0.67\n",
      "testing set, month 166, accuracy = 0.56\n",
      "testing set, month 166, AUC = 0.61\n",
      "testing set, month 167, accuracy = 0.61\n",
      "testing set, month 167, AUC = 0.64\n",
      "testing set, month 168, accuracy = 0.57\n",
      "testing set, month 168, AUC = 0.60\n",
      "testing set, month 169, accuracy = 0.57\n",
      "testing set, month 169, AUC = 0.60\n",
      "testing set, month 170, accuracy = 0.56\n",
      "testing set, month 170, AUC = 0.61\n",
      "testing set, month 171, accuracy = 0.52\n",
      "testing set, month 171, AUC = 0.52\n",
      "testing set, month 172, accuracy = 0.61\n",
      "testing set, month 172, AUC = 0.65\n",
      "testing set, month 173, accuracy = 0.57\n",
      "testing set, month 173, AUC = 0.61\n",
      "testing set, month 174, accuracy = 0.60\n",
      "testing set, month 174, AUC = 0.65\n",
      "testing set, month 175, accuracy = 0.54\n",
      "testing set, month 175, AUC = 0.56\n",
      "testing set, month 176, accuracy = 0.62\n",
      "testing set, month 176, AUC = 0.66\n",
      "testing set, month 177, accuracy = 0.61\n",
      "testing set, month 177, AUC = 0.64\n",
      "testing set, month 178, accuracy = 0.52\n",
      "testing set, month 178, AUC = 0.53\n",
      "testing set, month 179, accuracy = 0.59\n",
      "testing set, month 179, AUC = 0.62\n",
      "testing set, month 180, accuracy = 0.53\n",
      "testing set, month 180, AUC = 0.55\n",
      "testing set, month 181, accuracy = 0.54\n",
      "testing set, month 181, AUC = 0.56\n",
      "testing set, month 182, accuracy = 0.46\n",
      "testing set, month 182, AUC = 0.46\n",
      "testing set, month 183, accuracy = 0.53\n",
      "testing set, month 183, AUC = 0.55\n",
      "testing set, month 184, accuracy = 0.71\n",
      "testing set, month 184, AUC = 0.76\n",
      "testing set, month 185, accuracy = 0.53\n",
      "testing set, month 185, AUC = 0.54\n",
      "testing set, month 186, accuracy = 0.63\n",
      "testing set, month 186, AUC = 0.68\n",
      "testing set, month 187, accuracy = 0.54\n",
      "testing set, month 187, AUC = 0.55\n",
      "testing set, month 188, accuracy = 0.53\n",
      "testing set, month 188, AUC = 0.55\n",
      "testing set, month 189, accuracy = 0.46\n",
      "testing set, month 189, AUC = 0.45\n",
      "testing set, month 190, accuracy = 0.61\n",
      "testing set, month 190, AUC = 0.66\n",
      "testing set, month 191, accuracy = 0.71\n",
      "testing set, month 191, AUC = 0.77\n",
      "testing set, month 192, accuracy = 0.59\n",
      "testing set, month 192, AUC = 0.60\n",
      "testing set, month 193, accuracy = 0.55\n",
      "testing set, month 193, AUC = 0.57\n",
      "testing set, month 194, accuracy = 0.50\n",
      "testing set, month 194, AUC = 0.51\n",
      "testing set, month 195, accuracy = 0.69\n",
      "testing set, month 195, AUC = 0.74\n",
      "testing set, month 196, accuracy = 0.59\n",
      "testing set, month 196, AUC = 0.63\n",
      "testing set, month 197, accuracy = 0.60\n",
      "testing set, month 197, AUC = 0.68\n",
      "testing set, month 198, accuracy = 0.58\n",
      "testing set, month 198, AUC = 0.61\n",
      "testing set, month 199, accuracy = 0.51\n",
      "testing set, month 199, AUC = 0.53\n",
      "testing set, month 200, accuracy = 0.53\n",
      "testing set, month 200, AUC = 0.52\n",
      "testing set, month 201, accuracy = 0.62\n",
      "testing set, month 201, AUC = 0.66\n",
      "testing set, month 202, accuracy = 0.48\n",
      "testing set, month 202, AUC = 0.50\n",
      "testing set, month 203, accuracy = 0.57\n",
      "testing set, month 203, AUC = 0.61\n",
      "testing set, month 204, accuracy = 0.58\n",
      "testing set, month 204, AUC = 0.61\n",
      "testing set, month 205, accuracy = 0.52\n",
      "testing set, month 205, AUC = 0.55\n",
      "testing set, month 206, accuracy = 0.67\n",
      "testing set, month 206, AUC = 0.72\n",
      "testing set, month 207, accuracy = 0.59\n",
      "testing set, month 207, AUC = 0.62\n",
      "testing set, month 208, accuracy = 0.61\n",
      "testing set, month 208, AUC = 0.66\n",
      "testing set, month 209, accuracy = 0.59\n",
      "testing set, month 209, AUC = 0.65\n",
      "testing set, month 210, accuracy = 0.57\n",
      "testing set, month 210, AUC = 0.60\n",
      "testing set, month 211, accuracy = 0.56\n",
      "testing set, month 211, AUC = 0.60\n",
      "testing set, month 212, accuracy = 0.66\n",
      "testing set, month 212, AUC = 0.72\n",
      "testing set, month 213, accuracy = 0.61\n",
      "testing set, month 213, AUC = 0.65\n",
      "testing set, month 214, accuracy = 0.58\n",
      "testing set, month 214, AUC = 0.61\n",
      "testing set, month 215, accuracy = 0.61\n",
      "testing set, month 215, AUC = 0.66\n",
      "testing set, month 216, accuracy = 0.62\n",
      "testing set, month 216, AUC = 0.68\n",
      "testing set, month 217, accuracy = 0.53\n",
      "testing set, month 217, AUC = 0.53\n",
      "testing set, month 218, accuracy = 0.54\n",
      "testing set, month 218, AUC = 0.57\n",
      "testing set, month 219, accuracy = 0.69\n",
      "testing set, month 219, AUC = 0.75\n",
      "testing set, month 220, accuracy = 0.62\n",
      "testing set, month 220, AUC = 0.64\n",
      "testing set, month 221, accuracy = 0.61\n",
      "testing set, month 221, AUC = 0.66\n",
      "testing set, month 222, accuracy = 0.55\n",
      "testing set, month 222, AUC = 0.57\n",
      "testing set, month 223, accuracy = 0.56\n",
      "testing set, month 223, AUC = 0.60\n",
      "testing set, month 224, accuracy = 0.62\n",
      "testing set, month 224, AUC = 0.66\n",
      "testing set, month 225, accuracy = 0.55\n",
      "testing set, month 225, AUC = 0.58\n",
      "testing set, month 226, accuracy = 0.55\n",
      "testing set, month 226, AUC = 0.57\n",
      "testing set, month 227, accuracy = 0.53\n",
      "testing set, month 227, AUC = 0.52\n",
      "testing set, month 228, accuracy = 0.50\n",
      "testing set, month 228, AUC = 0.50\n",
      "testing set, month 229, accuracy = 0.50\n",
      "testing set, month 229, AUC = 0.50\n",
      "testing set, month 230, accuracy = 0.63\n",
      "testing set, month 230, AUC = 0.69\n",
      "testing set, month 231, accuracy = 0.58\n",
      "testing set, month 231, AUC = 0.59\n",
      "testing set, month 232, accuracy = 0.59\n",
      "testing set, month 232, AUC = 0.62\n",
      "testing set, month 233, accuracy = 0.53\n",
      "testing set, month 233, AUC = 0.55\n",
      "testing set, month 234, accuracy = 0.55\n",
      "testing set, month 234, AUC = 0.58\n",
      "testing set, month 235, accuracy = 0.54\n",
      "testing set, month 235, AUC = 0.55\n",
      "testing set, month 236, accuracy = 0.48\n",
      "testing set, month 236, AUC = 0.49\n",
      "testing set, month 237, accuracy = 0.53\n",
      "testing set, month 237, AUC = 0.56\n",
      "testing set, month 238, accuracy = 0.54\n",
      "testing set, month 238, AUC = 0.57\n",
      "testing set, month 239, accuracy = 0.52\n",
      "testing set, month 239, AUC = 0.54\n",
      "testing set, month 240, accuracy = 0.56\n",
      "testing set, month 240, AUC = 0.58\n",
      "testing set, month 241, accuracy = 0.55\n",
      "testing set, month 241, AUC = 0.58\n",
      "testing set, month 242, accuracy = 0.47\n",
      "testing set, month 242, AUC = 0.47\n",
      "testing set, month 243, accuracy = 0.66\n",
      "testing set, month 243, AUC = 0.72\n",
      "testing set, month 244, accuracy = 0.54\n",
      "testing set, month 244, AUC = 0.56\n",
      "testing set, month 245, accuracy = 0.54\n",
      "testing set, month 245, AUC = 0.56\n",
      "testing set, month 246, accuracy = 0.54\n",
      "testing set, month 246, AUC = 0.55\n",
      "testing set, month 247, accuracy = 0.64\n",
      "testing set, month 247, AUC = 0.70\n",
      "testing set, month 248, accuracy = 0.52\n",
      "testing set, month 248, AUC = 0.53\n",
      "testing set, month 249, accuracy = 0.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing set, month 249, AUC = 0.60\n",
      "testing set, month 250, accuracy = 0.60\n",
      "testing set, month 250, AUC = 0.63\n",
      "testing set, month 251, accuracy = 0.57\n",
      "testing set, month 251, AUC = 0.61\n",
      "testing set, month 252, accuracy = 0.64\n",
      "testing set, month 252, AUC = 0.69\n",
      "testing set, month 253, accuracy = 0.56\n",
      "testing set, month 253, AUC = 0.60\n",
      "testing set, month 254, accuracy = 0.61\n",
      "testing set, month 254, AUC = 0.63\n",
      "testing set, month 255, accuracy = 0.57\n",
      "testing set, month 255, AUC = 0.60\n",
      "testing set, month 256, accuracy = 0.45\n",
      "testing set, month 256, AUC = 0.44\n",
      "testing set, month 257, accuracy = 0.55\n",
      "testing set, month 257, AUC = 0.58\n",
      "testing set, month 258, accuracy = 0.57\n",
      "testing set, month 258, AUC = 0.61\n",
      "testing set, month 259, accuracy = 0.54\n",
      "testing set, month 259, AUC = 0.56\n",
      "testing set, month 260, accuracy = 0.52\n",
      "testing set, month 260, AUC = 0.53\n",
      "testing set, month 261, accuracy = 0.50\n",
      "testing set, month 261, AUC = 0.50\n",
      "testing set, month 262, accuracy = 0.46\n",
      "testing set, month 262, AUC = 0.45\n",
      "testing set, month 263, accuracy = 0.69\n",
      "testing set, month 263, AUC = 0.75\n",
      "testing set, month 264, accuracy = 0.47\n",
      "testing set, month 264, AUC = 0.46\n",
      "testing set, month 265, accuracy = 0.50\n",
      "testing set, month 265, AUC = 0.51\n",
      "testing set, month 266, accuracy = 0.49\n",
      "testing set, month 266, AUC = 0.48\n",
      "testing set, month 267, accuracy = 0.56\n",
      "testing set, month 267, AUC = 0.60\n",
      "testing set, month 268, accuracy = 0.62\n",
      "testing set, month 268, AUC = 0.68\n",
      "testing set, month 269, accuracy = 0.53\n",
      "testing set, month 269, AUC = 0.55\n",
      "##----------------------------------\n",
      "4.SIMPLE STRATEGY:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annual excess return = 0.31\n",
      "annual excess volatility = 0.25\n",
      "information ratio = 1.26\n"
     ]
    }
   ],
   "source": [
    "para = Para()\n",
    "print(\"BASIC INFORMATION:\")\n",
    "print(\"method = %s\"%para.method)\n",
    "print(\"evaluation_metrics = %s\"%para.evaluation_metrics)\n",
    "print(\"scroll times = %d\"%len(para.scroll_times))\n",
    "print('------------------------------------')\n",
    "\n",
    "\n",
    "for i_times in para.scroll_times:\n",
    "    ## 生成样本内数据集\n",
    "    #-- generate in-sample data\n",
    "    print('')\n",
    "    print(\"training times = %d\"%(i_times+1))\n",
    "    for i_month in para.month_in_sample:\n",
    "        #-- load csv\n",
    "        file_name = para.path_data + str(i_month+i_times*para.scroll_interval) + '.csv'\n",
    "        data_curr_month = pd.read_csv(file_name, header = 0)\n",
    "        para.n_stock = data_curr_month.shape[0]\n",
    "\n",
    "        #-- remove nan\n",
    "        data_curr_month = data_curr_month.dropna(axis=0)\n",
    "\n",
    "        #-- label data    \n",
    "        data_curr_month = label_data(data_curr_month)\n",
    "\n",
    "        #-- merge\n",
    "        if i_month == para.month_in_sample[0]: #-- first month\n",
    "            data_in_sample = data_curr_month\n",
    "        else:\n",
    "            data_in_sample = pd.concat((data_in_sample,data_curr_month), axis=0)\n",
    "    # 样本内数据集\n",
    "    #-- generate in-sample data\n",
    "    X_in_sample = data_in_sample.loc[:,'EP':'bias'] #提取出EP和bias两列的数据\n",
    "\n",
    "    #-- classification\n",
    "    if para.method in ['LOGI','XGBOOST-C']:\n",
    "        y_in_sample = data_in_sample.loc[:,'return_bin']\n",
    "\n",
    "    #-- regression\n",
    "    if para.method in ['LR']:\n",
    "        y_in_sample = data_in_sample.loc[:,'return']\n",
    "    \n",
    "    ## 划分训练集和验证集\n",
    "    #-- generate train and cv data\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    if para.percent_cv > 0:\n",
    "        X_train, X_cv, y_train, y_cv = train_test_split(X_in_sample, y_in_sample, test_size=para.percent_cv, random_state=para.seed)\n",
    "    else:\n",
    "        X_train, y_train = X_in_sample.copy(), y_in_sample.copy()\n",
    "        \n",
    "     \n",
    "    ## 主成分分析\n",
    "    '''\n",
    "    #-- pca\n",
    "    from sklearn import decomposition\n",
    "\n",
    "    pca = decomposition.PCA(n_components=0.95)\n",
    "    pca.fit(X_train)\n",
    "    X_train = pca.transform(X_train)\n",
    "\n",
    "    if para.percent_cv > 0:\n",
    "        X_cv = pca.transform(X_cv)\n",
    "    '''\n",
    "\n",
    "    ## 设置模型\n",
    "    #-- set model\n",
    "\n",
    "    #-- logistic regression\n",
    "    if para.method == 'LOGI':\n",
    "        from sklearn import linear_model\n",
    "        model = linear_model.LogisticRegression(C=para.logi_c)\n",
    "\n",
    "    #-- XGBoost Classifier\n",
    "    if para.method == 'XGBOOST-C':\n",
    "        from xgboost import XGBClassifier\n",
    "        model = XGBClassifier(random_state=para.seed,\n",
    "                              n_estimators=para.xgbc_n_estimators,\n",
    "                              learning_rate=para.xgbc_learning_rate,\n",
    "                              subsample=para.xgbc_subsample_C,\n",
    "                              max_depth=para.xgbc_max_depth)\n",
    "\n",
    "    #-- linear regression\n",
    "    if para.method == 'LR':\n",
    "        from sklearn import linear_model\n",
    "        model = linear_model.LinearRegression(fit_intercept=True)\n",
    "    \n",
    "    ## 训练模型，交叉验证\n",
    "    #-- train model, and perform cross validation\n",
    "    #-- classification\n",
    "    if para.method in ['LOGI','XGBOOST-C']:\n",
    "        model.fit(X_train,y_train)\n",
    "        #-- y_pred: binary format; y_score: continious format\n",
    "        y_pred_train = model.predict(X_train) \n",
    "        y_score_train = model.predict_proba(X_train)[:,1]\n",
    "\n",
    "        if para.percent_cv > 0:\n",
    "            y_pred_cv = model.predict(X_cv)\n",
    "            y_score_cv = model.predict_proba(X_cv)[:,1]\n",
    "\n",
    "    #-- regression\n",
    "    if para.method in ['LR']:\n",
    "        model.fit(X_train,y_train)\n",
    "        y_score_train = model.predict(X_train)\n",
    "\n",
    "        if para.percent_cv > 0:\n",
    "            y_score_cv = model.predict(X_cv)\n",
    "    \n",
    "    #-- 计算交叉验证的指标\n",
    "    print('##----------------------------------')\n",
    "    print('1.CROSS-VALIDATION:')\n",
    "    if(para.evaluation_metrics == 'cross_val_score'):\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        scores = cross_val_score(model,X_train,y_train,cv=5)\n",
    "        print(scores)\n",
    "    if(para.evaluation_metrics == 'cross_validate'):\n",
    "        from sklearn.model_selection import cross_validate\n",
    "        from sklearn.metrics import recall_score\n",
    "        scoring = ['precision_macro', 'recall_macro']\n",
    "        scores = cross_validate(model, X_train, y_train, scoring=scoring, cv=5)\n",
    "        print(sorted(scores.keys()))\n",
    "        print(scores['test_recall_macro'])\n",
    "    \n",
    "    '''\n",
    "    #-- 超参数调整\n",
    "    from matplotlib import pyplot as plt\n",
    "    from sklearn.datasets import make_hastie_10_2\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.metrics import make_scorer\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    \n",
    "    def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "        if useTrainCV:\n",
    "            xgb_param = alg.get_xgb_params()\n",
    "            xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "            cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "                metrics='auc', early_stopping_rounds=early_stopping_rounds, show_progress=False)\n",
    "            alg.set_params(n_estimators=cvresult.shape[0])\n",
    "\n",
    "        #Fit the algorithm on the data\n",
    "        alg.fit(dtrain[predictors], dtrain['Disbursed'],eval_metric='auc')\n",
    "\n",
    "        #Predict training set:\n",
    "        dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "        dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "\n",
    "        #Print model report:\n",
    "        print (\"\\nModel Report\")\n",
    "        print (\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['Disbursed'].values, dtrain_predictions))\n",
    "        print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['Disbursed'], dtrain_predprob))\n",
    "        feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "        feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')\n",
    "    \n",
    "    param_test1 = {\n",
    "     'max_depth':range(1,5,1),\n",
    "     'min_child_weight':range(1,6,2)\n",
    "    }\n",
    "    gsearch1 = GridSearchCV(\n",
    "        model, \n",
    "        param_grid = param_test1,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=4,\n",
    "        cv=5\n",
    "    )\n",
    "    gsearch1.fit(X_train,y_train)\n",
    "    \n",
    "    param_test2 = {\n",
    "     'max_depth':[4,5,6],\n",
    "     'min_child_weight':[2,3,4]\n",
    "    }\n",
    "    gsearch2 = GridSearchCV(\n",
    "        model, \n",
    "        param_grid = param_test1,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=4,\n",
    "        cv=5\n",
    "    )\n",
    "    gsearch2.fit(X_train,y_train)\n",
    "    \n",
    "    param_test3 = {\n",
    "     'gamma':[i/10.0 for i in range(0,5)]\n",
    "    }\n",
    "    gsearch3 = GridSearchCV(\n",
    "        model, \n",
    "        param_grid = param_test1,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=4,\n",
    "        cv=5\n",
    "    )\n",
    "    gsearch3.fit(X_train,y_train)\n",
    "    \n",
    "    mode2 = XGBClassifier(random_state=para.seed,\n",
    "                          n_estimators=para.xgbc_n_estimators,\n",
    "                          learning_rate=para.xgbc_learning_rate,\n",
    "                          subsample=para.xgbc_subsample_C,\n",
    "                          max_depth=\n",
    "                          min_child_weight=\n",
    "                          gamma=\n",
    "                          )\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #- 内部关联性分析\n",
    "    print('##----------------------------------')\n",
    "    print('2.INTERNAL DEPENDENCE:')\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import median_absolute_error\n",
    "    from sklearn.metrics import r2_score\n",
    "\n",
    "    mse = mean_squared_error(y_cv, y_score_cv)\n",
    "    print(\"Mean Squared Error(MSE) = %.6f\"%mse)\n",
    "    mae = median_absolute_error(y_cv, y_score_cv)\n",
    "    print(\"Median Absolute Error(MAE) = %.6f\"%mae)\n",
    "    r2 = r2_score(y_cv, y_score_cv) \n",
    "    print(\"R^2 score = %.6f\"%r2)\n",
    "    \n",
    "    \n",
    "    ## 样本外预测\n",
    "    #-- predict\n",
    "    y_true_test = pd.DataFrame([np.nan] * np.zeros((para.n_stock,para.month_test[-1])))\n",
    "    y_pred_test = pd.DataFrame([np.nan] * np.zeros((para.n_stock,para.month_test[-1])))\n",
    "    y_score_test = pd.DataFrame([np.nan] * np.zeros((para.n_stock,para.month_test[-1])))\n",
    "\n",
    "    for i_month in para.month_test:\n",
    "        #-- load \n",
    "        file_name = para.path_data + str(i_month) + '.csv'\n",
    "        data_curr_month = pd.read_csv(file_name, header = 0)\n",
    "\n",
    "        #-- remove nan  \n",
    "        data_curr_month = data_curr_month.dropna(axis=0)\n",
    "\n",
    "        #-- generate X\n",
    "        X_curr_month = data_curr_month.loc[:,'EP':'bias']\n",
    "\n",
    "        #-- pca\n",
    "        #X_curr_month = pca.transform(X_curr_month)\n",
    "\n",
    "        #-- predict and get predicted probability\n",
    "        #-- classification\n",
    "        if para.method in ['LOGI','XGBOOST-C']:\n",
    "            y_pred_curr_month = model.predict(X_curr_month)\n",
    "            y_score_curr_month = model.predict_proba(X_curr_month)[:,1]\n",
    "\n",
    "        #-- linear regression\n",
    "        if para.method in ['LR',]:\n",
    "            y_score_curr_month = model.predict(X_curr_month)\n",
    "\n",
    "        #-- store real and predicted return\n",
    "        y_true_test.iloc[data_curr_month.index,i_month-1] = data_curr_month['return'][data_curr_month.index]\n",
    "        if para.method in ['LOGI','XGBOOST-C']:\n",
    "            y_pred_test.iloc[data_curr_month.index,i_month-1] = y_pred_curr_month\n",
    "        y_score_test.iloc[data_curr_month.index,i_month-1] = y_score_curr_month\n",
    "\n",
    "    ## 模型评价\n",
    "    #-- evaluate\n",
    "    print('##----------------------------------')\n",
    "    print('3.MODOLE EVALUATION:')\n",
    "    if para.method in ['LOGI','XGBOOST-C']:\n",
    "        output_acc_auc = pd.DataFrame([np.nan] * np.zeros((para.month_test[-1],6)))\n",
    "\n",
    "        from sklearn import metrics\n",
    "\n",
    "        print('training set, accuracy = %.2f'%metrics.accuracy_score(y_train, y_pred_train))\n",
    "        print('training set, AUC = %.2f'%metrics.roc_auc_score(y_train, y_score_train))\n",
    "        output_acc_auc.iloc[para.month_in_sample[-1]-1,0] = metrics.accuracy_score(y_train, y_pred_train)\n",
    "        output_acc_auc.iloc[para.month_in_sample[-1]-1,3] = metrics.roc_auc_score(y_train, y_score_train)\n",
    "\n",
    "        if para.percent_cv > 0:\n",
    "            print('cv set, accuracy = %.2f'%metrics.accuracy_score(y_cv, y_pred_cv))\n",
    "            print('cv set, AUC = %.2f'%metrics.roc_auc_score(y_cv, y_score_cv))\n",
    "            output_acc_auc.iloc[para.month_in_sample[-1]-1,1] = metrics.accuracy_score(y_cv, y_pred_cv)\n",
    "            output_acc_auc.iloc[para.month_in_sample[-1]-1,4] = metrics.roc_auc_score(y_cv, y_score_cv)    \n",
    "\n",
    "        for i_month in para.month_test:\n",
    "            #-- 4 types of y\n",
    "            #-- y_true_*: true continious\n",
    "            #-- y_*: true binary\n",
    "            #-- y_pred_*: predicted binary\n",
    "            #-- y_score_*: predicted continious\n",
    "            y_true_curr_month = pd.DataFrame({'return':y_true_test.iloc[:,i_month-1]})\n",
    "            y_pred_curr_month = y_pred_test.iloc[:,i_month-1]\n",
    "            y_score_curr_month = y_score_test.iloc[:,i_month-1]\n",
    "\n",
    "            #-- remove nan\n",
    "            y_true_curr_month = y_true_curr_month.dropna(axis=0)\n",
    "\n",
    "            #-- label data\n",
    "            y_curr_month = label_data(y_true_curr_month)['return_bin']\n",
    "\n",
    "            #-- only select best and worst 30% data\n",
    "            y_pred_curr_month = y_pred_curr_month[y_curr_month.index]\n",
    "            y_score_curr_month = y_score_curr_month[y_curr_month.index]\n",
    "\n",
    "            print('testing set, month %d, accuracy = %.2f'%(i_month, metrics.accuracy_score(y_curr_month, y_pred_curr_month)))\n",
    "            print('testing set, month %d, AUC = %.2f'%(i_month, metrics.roc_auc_score(y_curr_month, y_score_curr_month)))\n",
    "            output_acc_auc.iloc[i_month-1,2] = metrics.accuracy_score(y_curr_month, y_pred_curr_month)\n",
    "            output_acc_auc.iloc[i_month-1,5] = metrics.roc_auc_score(y_curr_month, y_score_curr_month)\n",
    "\n",
    "    if para.method in ['LR']:\n",
    "        output_ic = pd.DataFrame([np.nan] * np.zeros((para.month_test[-1],3)))\n",
    "\n",
    "        y_train.index = range(len(y_train))\n",
    "        y_score_train = pd.Series(y_score_train)\n",
    "        print('training set, ic = %.2f'%y_train.corr(y_score_train))\n",
    "        output_ic.iloc[para.month_in_sample[-1]-1,0] = y_train.corr(y_score_train)\n",
    "\n",
    "        if para.percent_cv > 0:\n",
    "            y_cv.index = range(len(y_cv))\n",
    "            y_score_cv = pd.Series(y_score_cv)\n",
    "            print('cv set, ic = %.2f'%y_cv.corr(y_score_cv))\n",
    "            output_ic.iloc[para.month_in_sample[-1]-1,1] = y_cv.corr(y_score_cv)\n",
    "\n",
    "        for i_month in para.month_test:\n",
    "            y_true_curr_month = y_true_test.iloc[:,i_month-1]\n",
    "            y_score_curr_month = y_score_test.iloc[:,i_month-1]\n",
    "            print('testing set, month %d, ic = %.2f'%(i_month, y_true_curr_month.corr(y_score_curr_month)))\n",
    "            output_ic.iloc[i_month-1,2] = y_true_curr_month.corr(y_score_curr_month)\n",
    "\n",
    "    ## 简易回测\n",
    "    #-- simple strategy, select 50 stocks every month, equally weighted\n",
    "    print('##----------------------------------')\n",
    "    print('4.SIMPLE STRATEGY:')\n",
    "    para.n_stock_select = 50\n",
    "    strategy = pd.DataFrame({'return':[0] * (para.month_test[-1]+1),'value':[1] * (para.month_test[-1]+1)})\n",
    "\n",
    "    for i_month in para.month_test:\n",
    "        #-- get real and predicted return\n",
    "        y_true_curr_month = y_true_test.iloc[:,i_month-1]\n",
    "        y_score_curr_month = y_score_test.iloc[:,i_month-1]\n",
    "\n",
    "        #-- sort predicted return, and choose the best 50\n",
    "        y_score_curr_month = y_score_curr_month.sort_values(ascending=False)\n",
    "        index_select = y_score_curr_month[0:para.n_stock_select].index\n",
    "\n",
    "        #-- take the average return as the return of next month\n",
    "        strategy.loc[i_month-1,'return'] = np.mean(y_true_curr_month[index_select])\n",
    "\n",
    "    #-- compute the compund value of the strategy\n",
    "    strategy['value'] = (strategy['return'] + 1).cumprod()\n",
    "\n",
    "    #-- plot the value\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(para.month_test,strategy.loc[para.month_test,'value'],'r-')\n",
    "    plt.show()\n",
    "\n",
    "    #-- evaluation\n",
    "    ann_excess_return = np.mean(strategy.loc[para.month_test,'return']) * 12\n",
    "    ann_excess_vol = np.std(strategy.loc[para.month_test,'return']) * np.sqrt(12)\n",
    "    info_ratio = ann_excess_return/ann_excess_vol\n",
    "\n",
    "    print('annual excess return = %.2f'%ann_excess_return)\n",
    "    print('annual excess volatility = %.2f'%ann_excess_vol)\n",
    "    print('information ratio = %.2f'%info_ratio)\n",
    "\n",
    "    ## 保存结果\n",
    "    #-- save results\n",
    "    import os\n",
    "    os.makedirs(para.path_results,exist_ok=True)\n",
    "\n",
    "    filename = para.path_results + str(i_times) + 'dataTestYhat.csv'\n",
    "    y_score_test.to_csv(filename,header=0,index=False)\n",
    "\n",
    "    if para.method in ['LOGI','XGBOOST-C']:\n",
    "        filename = para.path_results + 'dataTestAccAuc.csv'\n",
    "        output_acc_auc.to_csv(filename,header=0,index=False)\n",
    "\n",
    "    if para.method in ['LR']:\n",
    "        filename = para.path_results + 'dataTestIC.csv'\n",
    "        output_ic.to_csv(filename,header=0,index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3bdf608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef compute_recall_and_auc(y_t, y_p):\\n    #混淆矩阵\\n    cnf_matrix=confusion_matrix(y_t,y_p)\\n    #设置numpy的打印精度\\n    np.set_printoptions(precision=2)\\n    recall_score = cnf_matrix[0,0]/(cnf_matrix[1,0]+cnf_matrix[0,0])\\n    \\n    #Roc曲线\\n    # https://www.cnblogs.com/gatherstars/p/6084696.html\\n    fpr, tpr,thresholds = roc_curve(y_t,y_p)\\n    roc_auc= auc(fpr,tpr)\\n    return recall_score , roc_auc\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def compute_recall_and_auc(y_t, y_p):\n",
    "    #混淆矩阵\n",
    "    cnf_matrix=confusion_matrix(y_t,y_p)\n",
    "    #设置numpy的打印精度\n",
    "    np.set_printoptions(precision=2)\n",
    "    recall_score = cnf_matrix[0,0]/(cnf_matrix[1,0]+cnf_matrix[0,0])\n",
    "    \n",
    "    #Roc曲线\n",
    "    # https://www.cnblogs.com/gatherstars/p/6084696.html\n",
    "    fpr, tpr,thresholds = roc_curve(y_t,y_p)\n",
    "    roc_auc= auc(fpr,tpr)\n",
    "    return recall_score , roc_auc\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31ff7b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef cross_validation_recall(x_train_data, y_train_data, c_param_range, models_dict, model_name):\\n    #使用K折交叉验证来寻找最优超参数\\n    fold=KFold(5,shuffle=False)\\n    # 构造超参数得分列表\\n    results_table = pd.DataFrame(index= range(len(c_param_range),2), columns = ['C_parameter','Mean recall score'])\\n    results_table['C_parameter'] = c_param_range\\n    \\n    recall_mean=[]\\n    # 循环使用每个超参数\\n    for c_param in c_param_range:\\n        recall_aucs=[]\\n        \\n        # 循环交叉集\\n        for i,train_index in enumerate(fold.split(y_train_data)):\\n            # 模型训练\\n            y_pred_undersample= models_dict[model_name](x_train_data,y_train_data, train_index, c_param)\\n            \\n            # 计算召回率和ROC曲线\\n            recall_auc, _=compute_recall_and_auc(y_train_data.iloc[train_index[1],:].values,y_pred_undersample)\\n            print(model_name,'第',i,'次：',recall_auc)\\n            recall_aucs.append(recall_auc)\\n        \\n        # auc取平均值作为这组超参数的分数\\n        recall_mean.append(np.mean(recall_aucs))\\n    \\n    results_table['Mean recall score'] = recall_mean\\n    # 得分最大的一组作为最优超参数，并返回\\n    best_c = results_table.loc[results_table['Mean recall score'].idxmax()]['C_parameter']\\n    return best_c\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def cross_validation_recall(x_train_data, y_train_data, c_param_range, models_dict, model_name):\n",
    "    #使用K折交叉验证来寻找最优超参数\n",
    "    fold=KFold(5,shuffle=False)\n",
    "    # 构造超参数得分列表\n",
    "    results_table = pd.DataFrame(index= range(len(c_param_range),2), columns = ['C_parameter','Mean recall score'])\n",
    "    results_table['C_parameter'] = c_param_range\n",
    "    \n",
    "    recall_mean=[]\n",
    "    # 循环使用每个超参数\n",
    "    for c_param in c_param_range:\n",
    "        recall_aucs=[]\n",
    "        \n",
    "        # 循环交叉集\n",
    "        for i,train_index in enumerate(fold.split(y_train_data)):\n",
    "            # 模型训练\n",
    "            y_pred_undersample= models_dict[model_name](x_train_data,y_train_data, train_index, c_param)\n",
    "            \n",
    "            # 计算召回率和ROC曲线\n",
    "            recall_auc, _=compute_recall_and_auc(y_train_data.iloc[train_index[1],:].values,y_pred_undersample)\n",
    "            print(model_name,'第',i,'次：',recall_auc)\n",
    "            recall_aucs.append(recall_auc)\n",
    "        \n",
    "        # auc取平均值作为这组超参数的分数\n",
    "        recall_mean.append(np.mean(recall_aucs))\n",
    "    \n",
    "    results_table['Mean recall score'] = recall_mean\n",
    "    # 得分最大的一组作为最优超参数，并返回\n",
    "    best_c = results_table.loc[results_table['Mean recall score'].idxmax()]['C_parameter']\n",
    "    return best_c\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89b62a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 不同的决策边界阈值\\n# 也是通过遍历调参的方式确定\\ndef decision_boundary(x_train_data, y_train_data, fold, best_c, bdry_dict, models_dict, model_name):\\n    bdry_range= [0.3,0.35,0.4,0.45,0.5]\\n    results_table = pd.DataFrame(index = range(len(bdry_ranges),2) , columns = ['Bdry_params','Mean recall score * auc'])\\n    results_table['Bdry_params']= bdry_ranges\\n    \\n    recall_mean=[]\\n    for bdry in bdry_ranges:\\n        recall_accs_aucs = []\\n        for iteration, indices in enumerate(fold.split(y_train_data)):\\n            y_pred_undersample = models_dict[model_name](x_train_data, y_train_data, indices, best_c, bdry)\\n            recall_acc, roc_auc = compute_recall_and_auc(y_train_data.iloc[indices[1],:].values, y_pred_undersample)\\n            \\n            # bdry_dict[model_name]是调用不同模型的计算公式\\n            recall_accs_aucs.append(bdry_dict[model_name](recall_acc, roc_auc))\\n        recall_mean.append(np.mean(recall_accs_aucs))\\n\\n    results_table['Mean recall score * auc'] = recall_mean\\n    best_bdry = results_table.loc[results_table['Mean recall score * auc'].idxmax()]['Bdry_params']\\n\\n    return best_bdry\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 不同的决策边界阈值\n",
    "# 也是通过遍历调参的方式确定\n",
    "def decision_boundary(x_train_data, y_train_data, fold, best_c, bdry_dict, models_dict, model_name):\n",
    "    bdry_range= [0.3,0.35,0.4,0.45,0.5]\n",
    "    results_table = pd.DataFrame(index = range(len(bdry_ranges),2) , columns = ['Bdry_params','Mean recall score * auc'])\n",
    "    results_table['Bdry_params']= bdry_ranges\n",
    "    \n",
    "    recall_mean=[]\n",
    "    for bdry in bdry_ranges:\n",
    "        recall_accs_aucs = []\n",
    "        for iteration, indices in enumerate(fold.split(y_train_data)):\n",
    "            y_pred_undersample = models_dict[model_name](x_train_data, y_train_data, indices, best_c, bdry)\n",
    "            recall_acc, roc_auc = compute_recall_and_auc(y_train_data.iloc[indices[1],:].values, y_pred_undersample)\n",
    "            \n",
    "            # bdry_dict[model_name]是调用不同模型的计算公式\n",
    "            recall_accs_aucs.append(bdry_dict[model_name](recall_acc, roc_auc))\n",
    "        recall_mean.append(np.mean(recall_accs_aucs))\n",
    "\n",
    "    results_table['Mean recall score * auc'] = recall_mean\n",
    "    best_bdry = results_table.loc[results_table['Mean recall score * auc'].idxmax()]['Bdry_params']\n",
    "\n",
    "    return best_bdry\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9335329b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef model(x,y,train, bdry_dict = None, best_c=None, best_bdry=None, models= None, mode=None):\\n    #训练阶段\\n    if train:\\n        #用不同的模型进行训练\\n        models_dict = {'knn' : knn_module, 'svm_rbf': svm_rbf_module, 'svm_poly': svm_poly_module,\\n                        'lr': lr_module, 'rf': rf_module}\\n        \\n        #knn中取不同的k值(超参数)\\n        c_param_range_knn=[3,5,7,9]\\n        #自定义cross_validation_recall，使用循环找出最适合的超参数。\\n        best_c_knn=cross_validation_recall(x,y, c_param_range_knn,models_dict, 'knn')\\n        \\n        # SVM-RBF中不同的参数\\n        c_param_range_svm_rbf=[0.01,0.1,1,10,100]\\n        best_c_svm_rbf = cross_validation_recall(x,y,c_param_range_svm_rbf, models_dict, 'svm_rbf')\\n        \\n        c_param_range_svm_poly = [[0.01, 2], [0.01, 3], [0.01, 4], [0.01, 5], [0.01, 6], [0.01, 7], [0.01, 8], [0.01, 9],\\n                                  [0.1, 2], [0.1, 3], [0.1, 4], [0.1, 5], [0.1, 6], [0.1, 7], [0.1, 8], [0.1, 9],\\n                                  [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 8], [1, 9],\\n                                  [10, 2], [10, 3], [10, 4], [10, 5], [10, 6], [10, 7], [10, 8], [10, 9],\\n                                  [100, 2], [100, 3], [100, 4], [100, 5], [100, 6], [100, 7], [100, 8], [100, 9]]\\n        \\n        best_c_svm_poly = cross_validation_recall(x,y, c_param_range_svm_poly, models_dict, 'svm_poly')\\n        \\n        # 逻辑回归当中的正则化强度\\n        c_param_range_lr=[0.01,0.1,1,10,100]\\n        best_c_lr = cross_validation_recall(x,y, c_param_range_lr, models_dict, 'lr')\\n        \\n        # 随机森林里调参\\n        c_param_range_rf = [2,5,10,15,20]\\n        best_c_rf= cross_validation_recall(X, y, c_param_range_rf, models_dict, 'rf')\\n        \\n        # 合并超参数\\n        best_c = [best_c_knn, best_c_svm_rbf, best_c_svm_poly, best_c_lr, best_c_rf, best_c]\\n        \\n        # 交叉验证确定合适的决策边界阈值\\n        fold = KFold(4,shuffle=True)\\n        \\n        # decision_boundary是一个计算决策边界的函数\\n        best_bdry_svm_rbf= decision_boundary(x, y, fold, best_c_svm_rbf, bdry_dict, models_dict, 'svm_rbf')\\n        best_bdry_svm_poly = decision_boundary(x, y, fold, best_c_svm_poly, bdry_dict, models_dict, 'svm_poly')\\n        best_bdry_lr = decision_boundary(x, y, fold, best_c_lr, bdry_dict, models_dict, 'lr')\\n        best_bdry_rf = decision_boundary(x, y, fold, best_c_rf, bdry_dict, models_dict, 'rf')\\n        best_bdry = [0.5, best_bdry_svm_rbf, best_bdry_svm_poly, best_bdry_lr, best_bdry_rf]\\n        \\n        # 最优参数建模\\n        knn = KNeighborsClassifier(n_neighbors = int(best_c_knn))\\n        knn.fit(x.values, y.values.ravel())\\n        \\n        svm_rbf = SVC(C=best_c_svm_rbf, probability = True)\\n        svm_rbf.fit(x.values, y.values.ravel())\\n        \\n        svm_poly = SVC(C=best_c_svm_poly[0], kernel = 'poly', degree = best_c_svm_poly[1], probability = True)\\n        svm_poly.fit(x.values, y.values.ravel())\\n\\n        lr = LogisticRegression(C = best_c_lr, penalty ='l1', warm_start = False)\\n        lr.fit(x.values, y.values.ravel())\\n\\n        rf = RandomForestClassifier(n_jobs=-1, n_estimators = 100, criterion = 'entropy', \\n                                    max_features = 'auto', max_depth = None, \\n                                    min_samples_split  = int(best_c_rf), random_state=0)\\n        rf.fit(x.values, y.values.ravel())\\n        \\n        models = [knn,svm_rbf,svm_poly, lr, rf]\\n        return best_c,best_bdry,models\\n    else:\\n        #预测阶段\\n        [knn, svm_rbf, svm_poly, lr, rf] = models\\n        [_, best_bdry_svm_rbf, best_bdry_svm_poly, best_bdry_lr, best_bdry_rf] = best_bdry\\n        \\n        # KNN\\n        y_pred_knn = knn.predict(x.values)\\n        # 用rbf核的SVM\\n        y_pred_svm_rbf = svm_rbf.predict_proba(x.values)[:,1] >= best_bdry_svm_rbf\\n        # 用多项式核的SVM\\n        y_pred_svm_poly = svm_poly.predict_proba(x.values)[:,1] >= best_bdry_svm_poly\\n        # LR\\n        y_pred_lr= lr.predict_proba(x.values)[:,1] >= best_bdry_lr\\n        # 随机森林\\n        y_pred_rf = rf.predict_proba(x.values)[:,1] >= best_bdry_rf\\n        \\n        x_of_three_models = {'knn' : y_pred_knn, 'svm_rbf' : y_pred_svm_rbf, 'svm_poly' : y_pred_svm_poly, 'lr' : y_pred_lr, 'rf': y_pred_rf}\\n        \\n        #得到5个模型的预测结果\\n        X_5_data = pd.DataFrame(data = x_of_three_models)\\n        \\n        # 进行投票机制，大于2票的为正样本\\n        y_prd= np.sum(x_5_data,axis=1)>=2\\n        \\n        y_pred_lr_controls = []\\n        params = [0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\\n\\n        # 投票器去产出最终结果\\n        for param in params:\\n            y_pred_lr_controls.append(lr.predict_proba(X.values)[:,1] >= param)\\n        return y_pred, y_pred_lr_controls, params\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def model(x,y,train, bdry_dict = None, best_c=None, best_bdry=None, models= None, mode=None):\n",
    "    #训练阶段\n",
    "    if train:\n",
    "        #用不同的模型进行训练\n",
    "        models_dict = {'knn' : knn_module, 'svm_rbf': svm_rbf_module, 'svm_poly': svm_poly_module,\n",
    "                        'lr': lr_module, 'rf': rf_module}\n",
    "        \n",
    "        #knn中取不同的k值(超参数)\n",
    "        c_param_range_knn=[3,5,7,9]\n",
    "        #自定义cross_validation_recall，使用循环找出最适合的超参数。\n",
    "        best_c_knn=cross_validation_recall(x,y, c_param_range_knn,models_dict, 'knn')\n",
    "        \n",
    "        # SVM-RBF中不同的参数\n",
    "        c_param_range_svm_rbf=[0.01,0.1,1,10,100]\n",
    "        best_c_svm_rbf = cross_validation_recall(x,y,c_param_range_svm_rbf, models_dict, 'svm_rbf')\n",
    "        \n",
    "        c_param_range_svm_poly = [[0.01, 2], [0.01, 3], [0.01, 4], [0.01, 5], [0.01, 6], [0.01, 7], [0.01, 8], [0.01, 9],\n",
    "                                  [0.1, 2], [0.1, 3], [0.1, 4], [0.1, 5], [0.1, 6], [0.1, 7], [0.1, 8], [0.1, 9],\n",
    "                                  [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 8], [1, 9],\n",
    "                                  [10, 2], [10, 3], [10, 4], [10, 5], [10, 6], [10, 7], [10, 8], [10, 9],\n",
    "                                  [100, 2], [100, 3], [100, 4], [100, 5], [100, 6], [100, 7], [100, 8], [100, 9]]\n",
    "        \n",
    "        best_c_svm_poly = cross_validation_recall(x,y, c_param_range_svm_poly, models_dict, 'svm_poly')\n",
    "        \n",
    "        # 逻辑回归当中的正则化强度\n",
    "        c_param_range_lr=[0.01,0.1,1,10,100]\n",
    "        best_c_lr = cross_validation_recall(x,y, c_param_range_lr, models_dict, 'lr')\n",
    "        \n",
    "        # 随机森林里调参\n",
    "        c_param_range_rf = [2,5,10,15,20]\n",
    "        best_c_rf= cross_validation_recall(X, y, c_param_range_rf, models_dict, 'rf')\n",
    "        \n",
    "        # 合并超参数\n",
    "        best_c = [best_c_knn, best_c_svm_rbf, best_c_svm_poly, best_c_lr, best_c_rf, best_c]\n",
    "        \n",
    "        # 交叉验证确定合适的决策边界阈值\n",
    "        fold = KFold(4,shuffle=True)\n",
    "        \n",
    "        # decision_boundary是一个计算决策边界的函数\n",
    "        best_bdry_svm_rbf= decision_boundary(x, y, fold, best_c_svm_rbf, bdry_dict, models_dict, 'svm_rbf')\n",
    "        best_bdry_svm_poly = decision_boundary(x, y, fold, best_c_svm_poly, bdry_dict, models_dict, 'svm_poly')\n",
    "        best_bdry_lr = decision_boundary(x, y, fold, best_c_lr, bdry_dict, models_dict, 'lr')\n",
    "        best_bdry_rf = decision_boundary(x, y, fold, best_c_rf, bdry_dict, models_dict, 'rf')\n",
    "        best_bdry = [0.5, best_bdry_svm_rbf, best_bdry_svm_poly, best_bdry_lr, best_bdry_rf]\n",
    "        \n",
    "        # 最优参数建模\n",
    "        knn = KNeighborsClassifier(n_neighbors = int(best_c_knn))\n",
    "        knn.fit(x.values, y.values.ravel())\n",
    "        \n",
    "        svm_rbf = SVC(C=best_c_svm_rbf, probability = True)\n",
    "        svm_rbf.fit(x.values, y.values.ravel())\n",
    "        \n",
    "        svm_poly = SVC(C=best_c_svm_poly[0], kernel = 'poly', degree = best_c_svm_poly[1], probability = True)\n",
    "        svm_poly.fit(x.values, y.values.ravel())\n",
    "\n",
    "        lr = LogisticRegression(C = best_c_lr, penalty ='l1', warm_start = False)\n",
    "        lr.fit(x.values, y.values.ravel())\n",
    "\n",
    "        rf = RandomForestClassifier(n_jobs=-1, n_estimators = 100, criterion = 'entropy', \n",
    "                                    max_features = 'auto', max_depth = None, \n",
    "                                    min_samples_split  = int(best_c_rf), random_state=0)\n",
    "        rf.fit(x.values, y.values.ravel())\n",
    "        \n",
    "        models = [knn,svm_rbf,svm_poly, lr, rf]\n",
    "        return best_c,best_bdry,models\n",
    "    else:\n",
    "        #预测阶段\n",
    "        [knn, svm_rbf, svm_poly, lr, rf] = models\n",
    "        [_, best_bdry_svm_rbf, best_bdry_svm_poly, best_bdry_lr, best_bdry_rf] = best_bdry\n",
    "        \n",
    "        # KNN\n",
    "        y_pred_knn = knn.predict(x.values)\n",
    "        # 用rbf核的SVM\n",
    "        y_pred_svm_rbf = svm_rbf.predict_proba(x.values)[:,1] >= best_bdry_svm_rbf\n",
    "        # 用多项式核的SVM\n",
    "        y_pred_svm_poly = svm_poly.predict_proba(x.values)[:,1] >= best_bdry_svm_poly\n",
    "        # LR\n",
    "        y_pred_lr= lr.predict_proba(x.values)[:,1] >= best_bdry_lr\n",
    "        # 随机森林\n",
    "        y_pred_rf = rf.predict_proba(x.values)[:,1] >= best_bdry_rf\n",
    "        \n",
    "        x_of_three_models = {'knn' : y_pred_knn, 'svm_rbf' : y_pred_svm_rbf, 'svm_poly' : y_pred_svm_poly, 'lr' : y_pred_lr, 'rf': y_pred_rf}\n",
    "        \n",
    "        #得到5个模型的预测结果\n",
    "        X_5_data = pd.DataFrame(data = x_of_three_models)\n",
    "        \n",
    "        # 进行投票机制，大于2票的为正样本\n",
    "        y_prd= np.sum(x_5_data,axis=1)>=2\n",
    "        \n",
    "        y_pred_lr_controls = []\n",
    "        params = [0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "\n",
    "        # 投票器去产出最终结果\n",
    "        for param in params:\n",
    "            y_pred_lr_controls.append(lr.predict_proba(X.values)[:,1] >= param)\n",
    "        return y_pred, y_pred_lr_controls, params\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7557f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
